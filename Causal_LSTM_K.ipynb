{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Causal LSTM K.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0kvdIrjoQc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "5fc3d0f2-9fcd-47b0-c92b-74a08339fb90"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 26.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.17.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.33.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqssfFSu8zrc",
        "colab_type": "text"
      },
      "source": [
        "# Inizio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFy6MXDenJJO",
        "colab_type": "code",
        "outputId": "dac4e95e-ee1b-4db5-bcfe-7cbc21f26fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import activations\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.layers.recurrent import _generate_dropout_mask\n",
        "from keras.layers.recurrent import _standardize_args\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "from keras.engine.base_layer import InputSpec, Layer\n",
        "from keras.utils import conv_utils\n",
        "from keras.legacy import interfaces\n",
        "from keras.legacy.layers import Recurrent, ConvRecurrent2D\n",
        "from keras.layers.recurrent import RNN\n",
        "from keras.utils.generic_utils import has_arg\n",
        "from keras.utils.generic_utils import to_list\n",
        "from keras.utils.generic_utils import transpose_shape\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfMfIGjDd_7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvRNN2D(keras.layers.recurrent.RNN):\n",
        "    \"\"\"Base class for convolutional-recurrent layers.\n",
        "    # Arguments\n",
        "        cell: A RNN cell instance. A RNN cell is a class that has:\n",
        "            - a `call(input_at_t, states_at_t)` method, returning\n",
        "              `(output_at_t, states_at_t_plus_1)`. The call method of the\n",
        "              cell can also take the optional argument `constants`, see\n",
        "              section \"Note on passing external constants\" below.\n",
        "            - a `state_size` attribute. This can be a single integer (single state)\n",
        "              in which case it is the number of channels of the recurrent state\n",
        "              (which should be the same as the number of channels of the cell\n",
        "              output). This can also be a list/tuple of integers\n",
        "              (one size per state). In this case, the first entry (`state_size[0]`)\n",
        "              should be the same as the size of the cell output.\n",
        "        return_sequences: Boolean. Whether to return the last output.\n",
        "            in the output sequence, or the full sequence.\n",
        "        return_state: Boolean. Whether to return the last state\n",
        "            in addition to the output.\n",
        "        go_backwards: Boolean (default False).\n",
        "            If True, process the input sequence backwards and return the\n",
        "            reversed sequence.\n",
        "        stateful: Boolean (default False). If True, the last state\n",
        "            for each sample at index i in a batch will be used as initial\n",
        "            state for the sample of index i in the following batch.\n",
        "        input_shape: Use this argument to specify the shape of the\n",
        "            input when this layer is the first one in a model.\n",
        "    # Input shape\n",
        "        5D tensor with shape:\n",
        "        `(samples, timesteps, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 5D tensor with shape:\n",
        "        `(samples, timesteps, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        - if `return_state`: a list of tensors. The first tensor is\n",
        "            the output. The remaining tensors are the last states,\n",
        "            each 5D tensor with shape:\n",
        "            `(samples, timesteps,\n",
        "              filters, new_rows, new_cols)` if data_format='channels_first'\n",
        "            or 5D tensor with shape:\n",
        "            `(samples, timesteps,\n",
        "              new_rows, new_cols, filters)` if data_format='channels_last'.\n",
        "            `rows` and `cols` values might have changed due to padding.\n",
        "        - if `return_sequences`: 5D tensor with shape:\n",
        "            `(samples, timesteps,\n",
        "              filters, new_rows, new_cols)` if data_format='channels_first'\n",
        "            or 5D tensor with shape:\n",
        "            `(samples, timesteps,\n",
        "              new_rows, new_cols, filters)` if data_format='channels_last'.\n",
        "        - else, 4D tensor with shape:\n",
        "            `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n",
        "            or 4D tensor with shape:\n",
        "            `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
        "    # Masking\n",
        "        This layer supports masking for input data with a variable number\n",
        "        of timesteps. To introduce masks to your data,\n",
        "        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n",
        "        set to `True`.\n",
        "    # Note on using statefulness in RNNs\n",
        "        You can set RNN layers to be 'stateful', which means that the states\n",
        "        computed for the samples in one batch will be reused as initial states\n",
        "        for the samples in the next batch. This assumes a one-to-one mapping\n",
        "        between samples in different successive batches.\n",
        "        To enable statefulness:\n",
        "            - specify `stateful=True` in the layer constructor.\n",
        "            - specify a fixed batch size for your model, by passing\n",
        "                 - if sequential model:\n",
        "                    `batch_input_shape=(...)` to the first layer in your model.\n",
        "                 - if functional model with 1 or more Input layers:\n",
        "                    `batch_shape=(...)` to all the first layers in your model.\n",
        "                    This is the expected shape of your inputs\n",
        "                    *including the batch size*.\n",
        "                    It should be a tuple of integers, e.g. `(32, 10, 100, 100, 32)`.\n",
        "                    Note that the number of rows and columns should be specified too.\n",
        "            - specify `shuffle=False` when calling fit().\n",
        "        To reset the states of your model, call `.reset_states()` on either\n",
        "        a specific layer, or on your entire model.\n",
        "    # Note on specifying the initial state of RNNs\n",
        "        You can specify the initial state of RNN layers symbolically by\n",
        "        calling them with the keyword argument `initial_state`. The value of\n",
        "        `initial_state` should be a tensor or list of tensors representing\n",
        "        the initial state of the RNN layer.\n",
        "        You can specify the initial state of RNN layers numerically by\n",
        "        calling `reset_states` with the keyword argument `states`. The value of\n",
        "        `states` should be a numpy array or list of numpy arrays representing\n",
        "        the initial state of the RNN layer.\n",
        "    # Note on passing external constants to RNNs\n",
        "        You can pass \"external\" constants to the cell using the `constants`\n",
        "        keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n",
        "        requires that the `cell.call` method accepts the same keyword argument\n",
        "        `constants`. Such constants can be used to condition the cell\n",
        "        transformation on additional static inputs (not changing over time),\n",
        "        a.k.a. an attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cell,\n",
        "                 return_sequences=False,\n",
        "                 return_state=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        if unroll:\n",
        "            raise TypeError('Unrolling isn\\'t possible with '\n",
        "                            'convolutional RNNs.')\n",
        "        if isinstance(cell, (list, tuple)):\n",
        "            # The StackedConvRNN2DCells isn't implemented yet.\n",
        "            raise TypeError('It is not possible at the moment to'\n",
        "                            'stack convolutional cells.')\n",
        "        super(ConvRNN2D, self).__init__(cell,\n",
        "                                        return_sequences,\n",
        "                                        return_state,\n",
        "                                        go_backwards,\n",
        "                                        stateful,\n",
        "                                        unroll,\n",
        "                                        **kwargs)\n",
        "        self.input_spec = [InputSpec(ndim=5)]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape = input_shape[0]\n",
        "\n",
        "        cell = self.cell\n",
        "        if cell.data_format == 'channels_first':\n",
        "            rows = input_shape[3]\n",
        "            cols = input_shape[4]\n",
        "        elif cell.data_format == 'channels_last':\n",
        "            rows = input_shape[2]\n",
        "            cols = input_shape[3]\n",
        "        rows = conv_utils.conv_output_length(rows,\n",
        "                                             cell.kernel_size[0],\n",
        "                                             padding=cell.padding,\n",
        "                                             stride=cell.strides[0],\n",
        "                                             dilation=cell.dilation_rate[0])\n",
        "        cols = conv_utils.conv_output_length(cols,\n",
        "                                             cell.kernel_size[1],\n",
        "                                             padding=cell.padding,\n",
        "                                             stride=cell.strides[1],\n",
        "                                             dilation=cell.dilation_rate[1])\n",
        "\n",
        "        output_shape = input_shape[:2] + (rows, cols, cell.filters)\n",
        "        output_shape = transpose_shape(output_shape, cell.data_format,\n",
        "                                       spatial_axes=(2, 3))\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            output_shape = output_shape[:1] + output_shape[2:]\n",
        "\n",
        "        if self.return_state:\n",
        "            output_shape = [output_shape]\n",
        "            base = (input_shape[0], rows, cols, cell.filters)\n",
        "            base = transpose_shape(base, cell.data_format, spatial_axes=(1, 2))\n",
        "            output_shape += [base[:] for _ in range(2)]\n",
        "        return output_shape\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Note input_shape will be list of shapes of initial states and\n",
        "        # constants if these are passed in __call__.\n",
        "        if self._num_constants is not None:\n",
        "            constants_shape = input_shape[-self._num_constants:]\n",
        "        else:\n",
        "            constants_shape = None\n",
        "\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape = input_shape[0]\n",
        "\n",
        "        batch_size = input_shape[0] if self.stateful else None\n",
        "        self.input_spec[0] = InputSpec(shape=(batch_size, None) + input_shape[2:5])\n",
        "\n",
        "        # allow cell (if layer) to build before we set or validate state_spec\n",
        "        if isinstance(self.cell, Layer):\n",
        "            step_input_shape = (input_shape[0],) + input_shape[2:]\n",
        "            if constants_shape is not None:\n",
        "                self.cell.build([step_input_shape] + constants_shape)\n",
        "            else:\n",
        "                self.cell.build(step_input_shape)\n",
        "\n",
        "        # set or validate state_spec\n",
        "        if hasattr(self.cell.state_size, '__len__'):\n",
        "            state_size = list(self.cell.state_size)\n",
        "        else:\n",
        "            state_size = [self.cell.state_size]\n",
        "\n",
        "        if self.state_spec is not None:\n",
        "            # initial_state was passed in call, check compatibility\n",
        "            if self.cell.data_format == 'channels_first':\n",
        "                ch_dim = 1\n",
        "            elif self.cell.data_format == 'channels_last':\n",
        "                ch_dim = 3\n",
        "            if not [spec.shape[ch_dim] for spec in self.state_spec] == state_size:\n",
        "                raise ValueError(\n",
        "                    'An initial_state was passed that is not compatible with '\n",
        "                    '`cell.state_size`. Received `state_spec`={}; '\n",
        "                    'However `cell.state_size` is '\n",
        "                    '{}'.format([spec.shape for spec in self.state_spec],\n",
        "                                self.cell.state_size))\n",
        "        else:\n",
        "            if self.cell.data_format == 'channels_first':\n",
        "                self.state_spec = [InputSpec(shape=(None, dim, None, None))\n",
        "                                   for dim in state_size]\n",
        "            elif self.cell.data_format == 'channels_last':\n",
        "                self.state_spec = [InputSpec(shape=(None, None, None, dim))\n",
        "                                   for dim in state_size]\n",
        "        if self.stateful:\n",
        "            self.reset_states()\n",
        "        self.built = True\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        # (samples, timesteps, rows, cols, filters)\n",
        "        initial_state = K.zeros_like(inputs)\n",
        "        # (samples, rows, cols, filters)\n",
        "        initial_state = K.sum(initial_state, axis=1)\n",
        "        shape = list(self.cell.kernel_shape)\n",
        "        shape[-1] = self.cell.filters\n",
        "\n",
        "        if K.backend() == 'tensorflow':\n",
        "            # We need to force this to be a tensor\n",
        "            # and not a variable, to avoid variable initialization\n",
        "            # issues.\n",
        "            import tensorflow as tf\n",
        "            kernel = tf.zeros(tuple(shape))\n",
        "        else:\n",
        "            kernel = K.zeros(tuple(shape))\n",
        "        initial_state = self.cell.input_conv(initial_state,\n",
        "                                             kernel,\n",
        "                                             padding=self.cell.padding)\n",
        "        # Fix for Theano because it needs\n",
        "        # K.int_shape to work in call() with initial_state.\n",
        "        keras_shape = list(K.int_shape(inputs))\n",
        "        keras_shape.pop(1)\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            indices = 2, 3\n",
        "        else:\n",
        "            indices = 1, 2\n",
        "        for i, j in enumerate(indices):\n",
        "            keras_shape[j] = conv_utils.conv_output_length(\n",
        "                keras_shape[j],\n",
        "                shape[i],\n",
        "                padding=self.cell.padding,\n",
        "                stride=self.cell.strides[i],\n",
        "                dilation=self.cell.dilation_rate[i])\n",
        "        initial_state._keras_shape = keras_shape\n",
        "\n",
        "        if hasattr(self.cell.state_size, '__len__'):\n",
        "            return [initial_state for _ in self.cell.state_size]\n",
        "        else:\n",
        "            return [initial_state]\n",
        "\n",
        "    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n",
        "        inputs, initial_state, constants = _standardize_args(\n",
        "            inputs, initial_state, constants, self._num_constants)\n",
        "\n",
        "        if initial_state is None and constants is None:\n",
        "            return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n",
        "\n",
        "        # If any of `initial_state` or `constants` are specified and are Keras\n",
        "        # tensors, then add them to the inputs and temporarily modify the\n",
        "        # input_spec to include them.\n",
        "\n",
        "        additional_inputs = []\n",
        "        additional_specs = []\n",
        "        if initial_state is not None:\n",
        "            kwargs['initial_state'] = initial_state\n",
        "            additional_inputs += initial_state\n",
        "            self.state_spec = []\n",
        "            for state in initial_state:\n",
        "                try:\n",
        "                    shape = K.int_shape(state)\n",
        "                # Fix for Theano\n",
        "                except TypeError:\n",
        "                    shape = tuple(None for _ in range(K.ndim(state)))\n",
        "                self.state_spec.append(InputSpec(shape=shape))\n",
        "\n",
        "            additional_specs += self.state_spec\n",
        "        if constants is not None:\n",
        "            kwargs['constants'] = constants\n",
        "            additional_inputs += constants\n",
        "            self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n",
        "                                   for constant in constants]\n",
        "            self._num_constants = len(constants)\n",
        "            additional_specs += self.constants_spec\n",
        "        # at this point additional_inputs cannot be empty\n",
        "        for tensor in additional_inputs:\n",
        "            if K.is_keras_tensor(tensor) != K.is_keras_tensor(additional_inputs[0]):\n",
        "                raise ValueError('The initial state or constants of an RNN'\n",
        "                                 ' layer cannot be specified with a mix of'\n",
        "                                 ' Keras tensors and non-Keras tensors')\n",
        "\n",
        "        if K.is_keras_tensor(additional_inputs[0]):\n",
        "            # Compute the full input spec, including state and constants\n",
        "            full_input = [inputs] + additional_inputs\n",
        "            full_input_spec = self.input_spec + additional_specs\n",
        "            # Perform the call with temporarily replaced input_spec\n",
        "            original_input_spec = self.input_spec\n",
        "            self.input_spec = full_input_spec\n",
        "            output = super(ConvRNN2D, self).__call__(full_input, **kwargs)\n",
        "            self.input_spec = original_input_spec\n",
        "            return output\n",
        "        else:\n",
        "            return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n",
        "\n",
        "    def call(self,\n",
        "             inputs,\n",
        "             mask=None,\n",
        "             training=None,\n",
        "             initial_state=None,\n",
        "             constants=None):\n",
        "        # note that the .build() method of subclasses MUST define\n",
        "        # self.input_spec and self.state_spec with complete input shapes.\n",
        "        if isinstance(inputs, list):\n",
        "            inputs = inputs[0]\n",
        "        if initial_state is not None:\n",
        "            pass\n",
        "        elif self.stateful:\n",
        "            initial_state = self.states\n",
        "        else:\n",
        "            initial_state = self.get_initial_state(inputs)\n",
        "\n",
        "        if isinstance(mask, list):\n",
        "            mask = mask[0]\n",
        "\n",
        "        if len(initial_state) != len(self.states):\n",
        "            raise ValueError('Layer has ' + str(len(self.states)) +\n",
        "                             ' states but was passed ' +\n",
        "                             str(len(initial_state)) +\n",
        "                             ' initial states.')\n",
        "        timesteps = K.int_shape(inputs)[1]\n",
        "\n",
        "        kwargs = {}\n",
        "        if has_arg(self.cell.call, 'training'):\n",
        "            kwargs['training'] = training\n",
        "\n",
        "        if constants:\n",
        "            if not has_arg(self.cell.call, 'constants'):\n",
        "                raise ValueError('RNN cell does not support constants')\n",
        "\n",
        "            def step(inputs, states):\n",
        "                constants = states[-self._num_constants:]\n",
        "                states = states[:-self._num_constants]\n",
        "                return self.cell.call(inputs, states, constants=constants,\n",
        "                                      **kwargs)\n",
        "        else:\n",
        "            def step(inputs, states):\n",
        "                return self.cell.call(inputs, states, **kwargs)\n",
        "\n",
        "        last_output, outputs, states = K.rnn(step,\n",
        "                                             inputs,\n",
        "                                             initial_state,\n",
        "                                             constants=constants,\n",
        "                                             go_backwards=self.go_backwards,\n",
        "                                             mask=mask,\n",
        "                                             input_length=timesteps)\n",
        "        if self.stateful:\n",
        "            updates = []\n",
        "            for i in range(len(states)):\n",
        "                updates.append((self.states[i], states[i]))\n",
        "            self.add_update(updates, inputs)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            output = outputs\n",
        "        else:\n",
        "            output = last_output\n",
        "\n",
        "        # Properly set learning phase\n",
        "        if getattr(last_output, '_uses_learning_phase', False):\n",
        "            output._uses_learning_phase = True\n",
        "\n",
        "        if self.return_state:\n",
        "            states = to_list(states, allow_tuple=True)\n",
        "            return [output] + states\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def reset_states(self, states=None):\n",
        "        if not self.stateful:\n",
        "            raise AttributeError('Layer must be stateful.')\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        state_shape = self.compute_output_shape(input_shape)\n",
        "        if self.return_state:\n",
        "            state_shape = state_shape[0]\n",
        "        if self.return_sequences:\n",
        "            state_shape = state_shape[:1] + state_shape[2:]\n",
        "        if None in state_shape:\n",
        "            raise ValueError('If a RNN is stateful, it needs to know '\n",
        "                             'its batch size. Specify the batch size '\n",
        "                             'of your input tensors: \\n'\n",
        "                             '- If using a Sequential model, '\n",
        "                             'specify the batch size by passing '\n",
        "                             'a `batch_input_shape` '\n",
        "                             'argument to your first layer.\\n'\n",
        "                             '- If using the functional API, specify '\n",
        "                             'the time dimension by passing a '\n",
        "                             '`batch_shape` argument to your Input layer.\\n'\n",
        "                             'The same thing goes for the number of rows '\n",
        "                             'and columns.')\n",
        "\n",
        "        # helper function\n",
        "        def get_tuple_shape(nb_channels):\n",
        "            result = list(state_shape)\n",
        "            if self.cell.data_format == 'channels_first':\n",
        "                result[1] = nb_channels\n",
        "            elif self.cell.data_format == 'channels_last':\n",
        "                result[3] = nb_channels\n",
        "            else:\n",
        "                raise KeyError\n",
        "            return tuple(result)\n",
        "\n",
        "        # initialize state if None\n",
        "        if self.states[0] is None:\n",
        "            if hasattr(self.cell.state_size, '__len__'):\n",
        "                self.states = [K.zeros(get_tuple_shape(dim))\n",
        "                               for dim in self.cell.state_size]\n",
        "            else:\n",
        "                self.states = [K.zeros(get_tuple_shape(self.cell.state_size))]\n",
        "        elif states is None:\n",
        "            if hasattr(self.cell.state_size, '__len__'):\n",
        "                for state, dim in zip(self.states, self.cell.state_size):\n",
        "                    K.set_value(state, np.zeros(get_tuple_shape(dim)))\n",
        "            else:\n",
        "                K.set_value(self.states[0],\n",
        "                            np.zeros(get_tuple_shape(self.cell.state_size)))\n",
        "        else:\n",
        "            states = to_list(states, allow_tuple=True)\n",
        "            if len(states) != len(self.states):\n",
        "                raise ValueError('Layer ' + self.name + ' expects ' +\n",
        "                                 str(len(self.states)) + ' states, '\n",
        "                                 'but it received ' + str(len(states)) +\n",
        "                                 ' state values. Input received: ' +\n",
        "                                 str(states))\n",
        "            for index, (value, state) in enumerate(zip(states, self.states)):\n",
        "                if hasattr(self.cell.state_size, '__len__'):\n",
        "                    dim = self.cell.state_size[index]\n",
        "                else:\n",
        "                    dim = self.cell.state_size\n",
        "                if value.shape != get_tuple_shape(dim):\n",
        "                    raise ValueError('State ' + str(index) +\n",
        "                                     ' is incompatible with layer ' +\n",
        "                                     self.name + ': expected shape=' +\n",
        "                                     str(get_tuple_shape(dim)) +\n",
        "                                     ', found shape=' + str(value.shape))\n",
        "                # TODO: consider batch calls to `set_value`.\n",
        "                K.set_value(state, value)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPa4mqOMtOWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPSILON = 0.00001\n",
        "def tensor_layer_norm(x, state_name):\n",
        "    x_shape = x.get_shape()\n",
        "    dims = x_shape.ndims\n",
        "    params_shape = x_shape[-1:]\n",
        "    if dims == 4:\n",
        "        m, v = tf.nn.moments(x, [1,2,3], keep_dims=True)\n",
        "    elif dims == 5:\n",
        "        m, v = tf.nn.moments(x, [1,2,3,4], keep_dims=True)\n",
        "    else:\n",
        "        raise ValueError('input tensor for layer normalization must be rank 4 or 5.')\n",
        "    b = tf.get_variable(state_name+'b',initializer=tf.zeros(params_shape))\n",
        "    s = tf.get_variable(state_name+'s',initializer=tf.ones(params_shape))\n",
        "    x_tln = tf.nn.batch_normalization(x, m, v, b, s, EPSILON)\n",
        "    return x_tln"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ30NUb7772z",
        "colab_type": "text"
      },
      "source": [
        "# CausalLSTMCell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3km7gr2mAlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalLSTMCell(keras.layers.Layer):\n",
        "    def __init__(self, \n",
        "                 layer_name, \n",
        "                 filter_size, \n",
        "                 num_hidden_in, \n",
        "                 num_hidden_out,\n",
        "                 seq_shape, \n",
        "                 forget_bias=1.0, \n",
        "                 tln=False, \n",
        "                 initializer=0.001,\n",
        "                    #qua\n",
        "                 strides=(1, 1),\n",
        "                 padding='valid',\n",
        "                 data_format=None,\n",
        "                 dilation_rate=(1, 1),\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='hard_sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 unit_forget_bias=True,\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initialize the Causal LSTM cell.\n",
        "        Args:\n",
        "            layer_name: layer names for different lstm layers.\n",
        "            filter_size: int tuple thats the height and width of the filter.\n",
        "            num_hidden_in: number of units for input tensor.\n",
        "            num_hidden_out: number of units for output tensor.\n",
        "            seq_shape: shape of a sequence.\n",
        "            forget_bias: float, The bias added to forget gates.\n",
        "            tln: whether to apply tensor layer normalization\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_name = layer_name\n",
        "        self.filter_size = filter_size\n",
        "        self.num_hidden_in = num_hidden_in\n",
        "        self.num_hidden = num_hidden_out\n",
        "        self.batch = seq_shape[0]\n",
        "        self.height = seq_shape[2]\n",
        "        self.width = seq_shape[3]\n",
        "        self.layer_norm = tln\n",
        "        self._forget_bias = forget_bias\n",
        "        self.initializer = tf.random_uniform_initializer(-initializer,initializer)\n",
        "        self.state_size = (self.filter_size, self.filter_size)\n",
        "        #Qua\n",
        "        self.filters = num_hidden_in\n",
        "        self.kernel_size = (filter_size, filter_size)\n",
        "        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = K.normalize_data_format(data_format)\n",
        "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n",
        "                                                        'dilation_rate')\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.unit_forget_bias = unit_forget_bias\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "    \n",
        "        \n",
        "        \n",
        "\n",
        "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
        "                warnings.warn(\n",
        "                    'RNN dropout is no longer supported with the Theano backend '\n",
        "                    'due to technical limitations. '\n",
        "                    'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
        "                    'or use the TensorFlow backend.')\n",
        "                dropout = 0.\n",
        "                recurrent_dropout = 0.\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = (self.filters, self.filters)\n",
        "        self._dropout_mask = None\n",
        "        self._recurrent_dropout_mask = None\n",
        "\n",
        "    '''\n",
        "    def build(self, batch_input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"name\",\n",
        "            shape=[batch_input_shape[-1],self.units],\n",
        "            initializer=\"glorot_normal\"\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\", \n",
        "            shape=[self.units], initializer=\"zeros\")\n",
        "        super().build(batch_input_shape)\n",
        "    '''\n",
        "\n",
        "    def init_state(self):\n",
        "        return tf.zeros([self.batch, self.height, self.width, self.num_hidden],\n",
        "                        dtype=tf.float32)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n",
        "        self.kernel_shape = kernel_shape\n",
        "        recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      name='kernel',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=recurrent_kernel_shape,\n",
        "            initializer=self.recurrent_initializer,\n",
        "            name='recurrent_kernel',\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "        if self.use_bias:\n",
        "            if self.unit_forget_bias:\n",
        "                #@K.eager <-------------------------------------------------------------------------- QUAAA\n",
        "                def bias_initializer(_, *args, **kwargs):\n",
        "                    return K.concatenate([\n",
        "                        self.bias_initializer((self.filters,), *args, **kwargs),\n",
        "                        initializers.Ones()((self.filters,), *args, **kwargs),\n",
        "                        self.bias_initializer((self.filters * 2,), *args, **kwargs),\n",
        "                    ])\n",
        "            else:\n",
        "                bias_initializer = self.bias_initializer\n",
        "            self.bias = self.add_weight(shape=(self.filters * 4,),\n",
        "                                        name='bias',\n",
        "                                        initializer=bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.kernel_i = self.kernel[:, :, :, :self.filters]\n",
        "        self.recurrent_kernel_i = self.recurrent_kernel[:, :, :, :self.filters]\n",
        "        self.kernel_f = self.kernel[:, :, :, self.filters: self.filters * 2]\n",
        "        self.recurrent_kernel_f = (\n",
        "            self.recurrent_kernel[:, :, :, self.filters: self.filters * 2])\n",
        "        self.kernel_c = self.kernel[:, :, :, self.filters * 2: self.filters * 3]\n",
        "        self.recurrent_kernel_c = (\n",
        "            self.recurrent_kernel[:, :, :, self.filters * 2: self.filters * 3])\n",
        "        self.kernel_o = self.kernel[:, :, :, self.filters * 3:]\n",
        "        self.recurrent_kernel_o = self.recurrent_kernel[:, :, :, self.filters * 3:]\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias_i = self.bias[:self.filters]\n",
        "            self.bias_f = self.bias[self.filters: self.filters * 2]\n",
        "            self.bias_c = self.bias[self.filters * 2: self.filters * 3]\n",
        "            self.bias_o = self.bias[self.filters * 3:]\n",
        "        else:\n",
        "            self.bias_i = None\n",
        "            self.bias_f = None\n",
        "            self.bias_c = None\n",
        "            self.bias_o = None\n",
        "        self.built = True\n",
        "\n",
        "    \n",
        "        \n",
        "    def call(self, x, h, c, m):\n",
        "        if h is None:\n",
        "            h = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden],\n",
        "                         dtype=tf.float32)\n",
        "        if c is None:\n",
        "            c = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden],\n",
        "                         dtype=tf.float32)\n",
        "        if m is None:\n",
        "            m = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden_in],\n",
        "                         dtype=tf.float32)\n",
        "\n",
        "        with tf.variable_scope(self.layer_name):\n",
        "            h_cc = tf.keras.layers.Conv2D(\n",
        "                h, self.num_hidden*4,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='temporal_state_transition')\n",
        "            c_cc = tf.keras.layers.Conv2D(\n",
        "                c, self.num_hidden*3,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='temporal_memory_transition')\n",
        "            m_cc = tf.keras.layers.Conv2D(\n",
        "                m, self.num_hidden*3,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='spatial_memory_transition')\n",
        "            if self.layer_norm:\n",
        "                h_cc = tensor_layer_norm(h_cc, 'h2c')\n",
        "                c_cc = tensor_layer_norm(c_cc, 'c2c')\n",
        "                m_cc = tensor_layer_norm(m_cc, 'm2m')\n",
        "\n",
        "            i_h, g_h, f_h, o_h = tf.split(h_cc, 4, 3)\n",
        "            i_c, g_c, f_c = tf.split(c_cc, 3, 3)\n",
        "            i_m, f_m, m_m = tf.split(m_cc, 3, 3)\n",
        "\n",
        "            if x is None:\n",
        "                i = tf.sigmoid(i_h + i_c)\n",
        "                f = tf.sigmoid(f_h + f_c + self._forget_bias)\n",
        "                g = tf.tanh(g_h + g_c)\n",
        "            else:\n",
        "                x_cc = tf.keras.layers.Conv2D(\n",
        "                    x, self.num_hidden*7,\n",
        "                    self.filter_size, 1, padding='same',\n",
        "                    kernel_initializer=self.initializer,\n",
        "                    name='input_to_state')\n",
        "                if self.layer_norm:\n",
        "                    x_cc = tensor_layer_norm(x_cc, 'x2c')\n",
        "\n",
        "                i_x, g_x, f_x, o_x, i_x_, g_x_, f_x_ = tf.split(x_cc, 7, 3)\n",
        "\n",
        "                i = tf.sigmoid(i_x + i_h + i_c)\n",
        "                f = tf.sigmoid(f_x + f_h + f_c + self._forget_bias)\n",
        "                g = tf.tanh(g_x + g_h + g_c)\n",
        "\n",
        "            c_new = f * c + i * g\n",
        "\n",
        "            c2m = tf.keras.layers.Conv2D(\n",
        "                c_new, self.num_hidden*4,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='c2m')\n",
        "            if self.layer_norm:\n",
        "                c2m = tensor_layer_norm(c2m, 'c2m')\n",
        "\n",
        "            i_c, g_c, f_c, o_c = tf.split(c2m, 4, 3)\n",
        "\n",
        "            if x is None:\n",
        "                ii = tf.sigmoid(i_c + i_m)\n",
        "                ff = tf.sigmoid(f_c + f_m + self._forget_bias)\n",
        "                gg = tf.tanh(g_c)\n",
        "            else:\n",
        "                ii = tf.sigmoid(i_c + i_x_ + i_m)\n",
        "                ff = tf.sigmoid(f_c + f_x_ + f_m + self._forget_bias)\n",
        "                gg = tf.tanh(g_c + g_x_)\n",
        "\n",
        "            m_new = ff * tf.tanh(m_m) + ii * gg\n",
        "\n",
        "            o_m = tf.keras.layers.Conv2D(\n",
        "                m_new, self.num_hidden,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='m_to_o')\n",
        "            if self.layer_norm:\n",
        "                o_m = tensor_layer_norm(o_m, 'm2o')\n",
        "\n",
        "            if x is None:\n",
        "                o = tf.tanh(o_h + o_c + o_m)\n",
        "            else:\n",
        "                o = tf.tanh(o_x + o_h + o_c + o_m)\n",
        "\n",
        "            cell = tf.concat([c_new, m_new],-1)\n",
        "            cell = tf.keras.layers.Conv2D(cell, self.num_hidden, 1, 1,\n",
        "                                    padding='same', name='memory_reduce')\n",
        "\n",
        "            h_new = o * tf.tanh(cell)\n",
        "\n",
        "            return h_new, c_new, m_new\n",
        "\n",
        "    def input_conv(self, x, w, b=None, padding='valid'):\n",
        "        conv_out = K.conv2d(x, w, strides=self.strides,\n",
        "                            padding=padding,\n",
        "                            data_format=self.data_format,\n",
        "                            dilation_rate=self.dilation_rate)\n",
        "        if b is not None:\n",
        "            conv_out = K.bias_add(conv_out, b,\n",
        "                                    data_format=self.data_format)\n",
        "        return conv_out\n",
        "\n",
        "    def recurrent_conv(self, x, w):\n",
        "        conv_out = K.conv2d(x, w, strides=(1, 1),\n",
        "                            padding='same',\n",
        "                            data_format=self.data_format)\n",
        "        return conv_out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'filters': self.filters,\n",
        "                  'kernel_size': self.kernel_size,\n",
        "                  'strides': self.strides,\n",
        "                  'padding': self.padding,\n",
        "                  'data_format': self.data_format,\n",
        "                  'dilation_rate': self.dilation_rate,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'recurrent_activation':\n",
        "                      activations.serialize(self.recurrent_activation),\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'kernel_initializer':\n",
        "                      initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer':\n",
        "                      initializers.serialize(self.recurrent_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'unit_forget_bias': self.unit_forget_bias,\n",
        "                  'kernel_regularizer':\n",
        "                      regularizers.serialize(self.kernel_regularizer),\n",
        "                  'recurrent_regularizer':\n",
        "                      regularizers.serialize(self.recurrent_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'kernel_constraint':\n",
        "                      constraints.serialize(self.kernel_constraint),\n",
        "                  'recurrent_constraint':\n",
        "                      constraints.serialize(self.recurrent_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'dropout': self.dropout,\n",
        "                  'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(ConvLSTM2DCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7kJYXNu738-",
        "colab_type": "text"
      },
      "source": [
        "# CausalLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKFr0R1ULjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalLSTM(ConvRNN2D):\n",
        "\n",
        "    def __init__(self,\n",
        "                 layer_name,\n",
        "                 filter_name,\n",
        "                 filter_size,\n",
        "                 num_hidden_in,\n",
        "                 num_hidden_out,\n",
        "                 seq_shape,\n",
        "                 return_sequences=False,\n",
        "                 go_backwards=False,\n",
        "                 stateful=False,\n",
        "                 forget_bias=1.0,\n",
        "                 tln=False,\n",
        "                 initializer=0.001,\n",
        "                 #Qua\n",
        "                  strides=(1, 1),\n",
        "                 padding='valid',\n",
        "                 data_format=None,\n",
        "                 dilation_rate=(1, 1),\n",
        "                 activation='tanh',\n",
        "                 recurrent_activation='hard_sigmoid',\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 unit_forget_bias=True,\n",
        "                 kernel_regularizer=None,\n",
        "                 recurrent_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 recurrent_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 dropout=0.,\n",
        "                 recurrent_dropout=0.,\n",
        "                 **kwargs):\n",
        "        \n",
        "        cell = CausalLSTMCell(\n",
        "            layer_name=layer_name,\n",
        "            filter_size=filter_size,\n",
        "            num_hidden_in=num_hidden_in,\n",
        "            num_hidden_out=num_hidden_out,\n",
        "            seq_shape=seq_shape,\n",
        "            forget_bias=forget_bias,\n",
        "            tln=tln,\n",
        "            initializer=initializer,\n",
        "            #qua\n",
        "            strides=strides,\n",
        "            padding=padding,\n",
        "            data_format=data_format,\n",
        "            dilation_rate=dilation_rate,\n",
        "            activation=activation,\n",
        "            recurrent_activation=recurrent_activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            recurrent_initializer=recurrent_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            unit_forget_bias=unit_forget_bias,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            recurrent_regularizer=recurrent_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            recurrent_constraint=recurrent_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            dropout=dropout,\n",
        "            recurrent_dropout=recurrent_dropout\n",
        "        )\n",
        "        super(CausalLSTM, self).__init__(cell,\n",
        "                                         return_sequences=return_sequences,\n",
        "                                         go_backwards=go_backwards,\n",
        "                                         stateful=stateful,\n",
        "                                         **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        \n",
        "        def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "            return super (CausalLSTM, self).call(inputs,\n",
        "                                                 mask=mask,\n",
        "                                                 training=training,\n",
        "                                                 initial_state=initial_state)\n",
        "\n",
        "        @property\n",
        "        def filters(self):\n",
        "            return self.cell.filters\n",
        "\n",
        "        @property\n",
        "        def kernel_size(self):\n",
        "            return self.cell.kernel_size\n",
        "\n",
        "        @property\n",
        "        def strides(self):\n",
        "            return self.cell.strides\n",
        "\n",
        "        @property\n",
        "        def padding(self):\n",
        "            return self.cell.padding\n",
        "\n",
        "        @property\n",
        "        def data_format(self):\n",
        "            return self.cell.data_format\n",
        "\n",
        "        @property\n",
        "        def dilation_rate(self):\n",
        "            return self.cell.dilation_rate\n",
        "\n",
        "        @property\n",
        "        def activation(self):\n",
        "            return self.cell.activation\n",
        "\n",
        "        @property\n",
        "        def recurrent_activation(self):\n",
        "            return self.cell.recurrent_activation\n",
        "\n",
        "        @property\n",
        "        def use_bias(self):\n",
        "            return self.cell.use_bias\n",
        "\n",
        "        @property\n",
        "        def kernel_initializer(self):\n",
        "            return self.cell.kernel_initializer\n",
        "\n",
        "        @property\n",
        "        def recurrent_initializer(self):\n",
        "            return self.cell.recurrent_initializer\n",
        "\n",
        "        @property\n",
        "        def bias_initializer(self):\n",
        "            return self.cell.bias_initializer\n",
        "\n",
        "        @property\n",
        "        def unit_forget_bias(self):\n",
        "            return self.cell.unit_forget_bias\n",
        "\n",
        "        @property\n",
        "        def kernel_regularizer(self):\n",
        "            return self.cell.kernel_regularizer\n",
        "\n",
        "        @property\n",
        "        def recurrent_regularizer(self):\n",
        "            return self.cell.recurrent_regularizer\n",
        "\n",
        "        @property\n",
        "        def bias_regularizer(self):\n",
        "            return self.cell.bias_regularizer\n",
        "\n",
        "        @property\n",
        "        def kernel_constraint(self):\n",
        "            return self.cell.kernel_constraint\n",
        "\n",
        "        @property\n",
        "        def recurrent_constraint(self):\n",
        "            return self.cell.recurrent_constraint\n",
        "\n",
        "        @property\n",
        "        def bias_constraint(self):\n",
        "            return self.cell.bias_constraint\n",
        "\n",
        "        @property\n",
        "        def dropout(self):\n",
        "            return self.cell.dropout\n",
        "\n",
        "        @property\n",
        "        def recurrent_dropout(self):\n",
        "            return self.cell.recurrent_dropout\n",
        "\n",
        "        def get_config(self):\n",
        "            config = {'filters': self.filters,\n",
        "                    'kernel_size': self.kernel_size,\n",
        "                    'strides': self.strides,\n",
        "                    'padding': self.padding,\n",
        "                    'data_format': self.data_format,\n",
        "                    'dilation_rate': self.dilation_rate,\n",
        "                    'activation': activations.serialize(self.activation),\n",
        "                    'recurrent_activation':\n",
        "                        activations.serialize(self.recurrent_activation),\n",
        "                    'use_bias': self.use_bias,\n",
        "                    'kernel_initializer':\n",
        "                        initializers.serialize(self.kernel_initializer),\n",
        "                    'recurrent_initializer':\n",
        "                        initializers.serialize(self.recurrent_initializer),\n",
        "                    'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                    'unit_forget_bias': self.unit_forget_bias,\n",
        "                    'kernel_regularizer':\n",
        "                        regularizers.serialize(self.kernel_regularizer),\n",
        "                    'recurrent_regularizer':\n",
        "                        regularizers.serialize(self.recurrent_regularizer),\n",
        "                    'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                    'activity_regularizer':\n",
        "                        regularizers.serialize(self.activity_regularizer),\n",
        "                    'kernel_constraint':\n",
        "                        constraints.serialize(self.kernel_constraint),\n",
        "                    'recurrent_constraint':\n",
        "                        constraints.serialize(self.recurrent_constraint),\n",
        "                    'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                    'dropout': self.dropout,\n",
        "                    'recurrent_dropout': self.recurrent_dropout}\n",
        "            base_config = super(ConvLSTM2D, self).get_config()\n",
        "            del base_config['cell']\n",
        "            return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "        @classmethod\n",
        "        def from_config(cls, config):\n",
        "            return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqsB6ngL71Ac",
        "colab_type": "text"
      },
      "source": [
        "# GHU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjyncnnQNRTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GHU(keras.layers.Layer):\n",
        "    def __init__(self, layer_name, filter_size, num_features, tln=False,\n",
        "                 initializer=0.001, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \"\"\"Initialize the Gradient Highway Unit.\n",
        "        \"\"\"\n",
        "        self.layer_name = layer_name\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self.layer_norm = tln\n",
        "        if initializer == -1:\n",
        "            self.initializer = None\n",
        "        else:\n",
        "            self.initializer = tf.random_uniform_initializer(-initializer,initializer)\n",
        "\n",
        "    def init_state(self, inputs, num_features):\n",
        "        dims = inputs.get_shape().ndims\n",
        "        if dims == 4:\n",
        "            batch = inputs.get_shape()[0]\n",
        "            height = inputs.get_shape()[1]\n",
        "            width = inputs.get_shape()[2]\n",
        "        else:\n",
        "            raise ValueError('input tensor should be rank 4.')\n",
        "        return tf.zeros([batch, height, width, num_features], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x, z):\n",
        "        if z is None:\n",
        "            z = self.init_state(x, self.num_features)\n",
        "        with tf.variable_scope(self.layer_name):\n",
        "            z_concat = tf.keras.layers.Conv2D(\n",
        "                z, self.num_features*2,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='state_to_state')\n",
        "            if self.layer_norm:\n",
        "                z_concat = tensor_layer_norm(z_concat, 'state_to_state')\n",
        "\n",
        "            x_concat = tf.keras.layers.Conv2D(\n",
        "                x, self.num_features*2,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='input_to_state')\n",
        "            if self.layer_norm:\n",
        "                x_concat = tensor_layer_norm(x_concat, 'input_to_state')\n",
        "\n",
        "            gates = tf.add(x_concat, z_concat)\n",
        "            p, u = tf.split(gates, 2, 3)\n",
        "            p = tf.nn.tanh(p)\n",
        "            u = tf.nn.sigmoid(u)\n",
        "            z_new = u * p + (1-u) * z\n",
        "            return z_new\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgjmvA5reMx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Input\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAg30tTEvq9",
        "colab_type": "text"
      },
      "source": [
        "# Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNmcetj1eqjk",
        "colab_type": "code",
        "outputId": "1a2aa9d0-db8a-4306-c5d7-1f31cface1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Tesi Angelo Maiorano 160178/Codice'\n",
        "sys.path.append('/content/gdrive/My Drive/Tesi Angelo Maiorano 160178/Codice')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw4dz7d-erHx",
        "colab_type": "code",
        "outputId": "8c133bfa-f85d-45c8-d94d-cb7d58e1a6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_set = np.load('/content/gdrive/My Drive/Tesi Angelo Maiorano 160178/Codice/mnist_test_seq.npy')\n",
        "#train_set = np.load('/content/gdrive/My Drive/Tesi Angelo Maiorano 160178/Codice/Dataset/UCSD_Anomaly_Dataset.v1p2/UCSDped1/UCSD1_train.npy')\n",
        "#test_set = np.load('/content/gdrive/My Drive/Tesi Angelo Maiorano 160178/Codice/Dataset/UCSD_Anomaly_Dataset.v1p2/UCSDped1/UCSD1_test.npy')\n",
        "print('Tipo di dato ',type(train_set))\n",
        "print('Data shape: ', train_set.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tipo di dato  <class 'numpy.ndarray'>\n",
            "Data shape:  (20, 10000, 64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVDywNbd5Lny",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auoaI_Rme129",
        "colab_type": "code",
        "outputId": "39b0894e-6b54-446d-fc10-a91640ed3d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inp = Input(shape=(None, 64, 64, 1))\n",
        "\n",
        "caus = CausalLSTM(layer_name=\"caus\", \n",
        "                  filter_name=\"filt\",\n",
        "                  filter_size=5,\n",
        "                  num_hidden_in=64,\n",
        "                  num_hidden_out=128,\n",
        "                  seq_shape=[30, 64, 64, 1])(inp)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-abb1de39437f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mnum_hidden_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mnum_hidden_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                   seq_shape=[30, 64, 64, 1])(inp)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-61f147a0d700>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvRNN2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-61f147a0d700>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    347\u001b[0m                                              \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                                              \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                                              input_length=timesteps)\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length)\u001b[0m\n\u001b[1;32m   3186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3187\u001b[0m         \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3188\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3189\u001b[0m         output_ta = tensor_array_ops.TensorArray(\n\u001b[1;32m   3190\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-61f147a0d700>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         last_output, outputs, states = K.rnn(step,\n",
            "\u001b[0;31mTypeError\u001b[0m: call() missing 2 required positional arguments: 'c' and 'm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXAV3ASFfoiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.constant(train_set)\n",
        "model = keras.Model(inputs=t, outputs = t)\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.004, rho=0.9, epsilon=None, decay=0.9)\n",
        "model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXyll1KcJQss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}