{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Causal LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brngl/CausalLSTM-Keras/blob/master/Keras_Causal_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "derY5ibBCqEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLRBQfINAphp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "7e252f1b-9859-42b4-8701-da918e0c6c0d"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import sys\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Tesi Angelo Maiorano 160178/Codice/predrnn-pp-master'\n",
        "\n",
        "sys.path.append(root_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM23pK9KAozu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "46eb0d4a-16de-4d23-aad2-0d376d9a8700"
      },
      "source": [
        "!drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "!ls\n",
        "%cd 'gdrive'\n",
        "%cd 'My Drive'\n",
        "%cd 'Tesi Angelo Maiorano 160178'\n",
        "%cd 'Codice'\n",
        "%cd 'predrnn-pp-master'\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `\"/content/gdrive\",'\n",
            "/bin/bash: -c: line 0: `drive.mount(\"/content/gdrive\", force_remount=True)'\n",
            "gdrive\tsample_data\n",
            "/content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "[Errno 2] No such file or directory: 'Tesi Angelo Maiorano 160178'\n",
            "/content/gdrive/My Drive\n",
            "[Errno 2] No such file or directory: 'Codice'\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive/predrnn-pp-master\n",
            " checkpoints\t\t     etapred.txt\t\t prova.py\n",
            " checkpoints-pred\t     etarec.txt\t\t\t README.md\n",
            " ciaone.png\t\t     eta.txt\t\t\t results\n",
            "'Copia di losses_list.txt'   layers\t\t\t results-pred\n",
            " data\t\t\t     losses_list.txt\t\t train.py\n",
            " data_provider\t\t     nets\t\t\t utils\n",
            " data_provider-pred\t     new_training-set-shuf.npz\n",
            " eta.npy\t\t     os.path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSbcdJy7E6Pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "355fef7e-c258-4d27-beda-5020b87fe0b6"
      },
      "source": [
        "%cd ano-moving-mnist/\n",
        "%ls "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/predrnn-pp-master/data/ano-moving-mnist\n",
            "anom_test.npz             just-temporal.npz  new_training-set.npz\n",
            "anom_train.npz            k15-solouno.npz    \u001b[0m\u001b[01;34mtest-results\u001b[0m/\n",
            "anom_valid.npz            k300-entrambi.npz  uniqueanomovingmnist-test.npz\n",
            "equal_sfarfa_testset.npz  k30-entrambi.npz   valid_train.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqssfFSu8zrc",
        "colab_type": "text"
      },
      "source": [
        "# Inizio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFy6MXDenJJO",
        "colab_type": "code",
        "outputId": "e20fccb6-699d-418b-91ab-ff3ef48fbf94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.keras import activations\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras import constraints\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras.engine.base_layer import Layer\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "from tensorflow.python.keras.layers.recurrent import _standardize_args\n",
        "from tensorflow.python.keras.layers.recurrent import DropoutRNNCellMixin\n",
        "from tensorflow.python.keras.layers.recurrent import RNN\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo3FU2-mCwX8",
        "colab_type": "text"
      },
      "source": [
        "#Versione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-PD63lBCHgE",
        "colab_type": "code",
        "outputId": "18d6faa4-0de7-4086-bf9a-a1777a086252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5xeanN0fS3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvRNN2D(RNN):\n",
        "  \"\"\"Base class for convolutional-recurrent layers.\n",
        "  Arguments:\n",
        "    cell: A RNN cell instance. A RNN cell is a class that has:\n",
        "      - a `call(input_at_t, states_at_t)` method, returning\n",
        "        `(output_at_t, states_at_t_plus_1)`. The call method of the\n",
        "        cell can also take the optional argument `constants`, see\n",
        "        section \"Note on passing external constants\" below.\n",
        "      - a `state_size` attribute. This can be a single integer\n",
        "        (single state) in which case it is\n",
        "        the number of channels of the recurrent state\n",
        "        (which should be the same as the number of channels of the cell\n",
        "        output). This can also be a list/tuple of integers\n",
        "        (one size per state). In this case, the first entry\n",
        "        (`state_size[0]`) should be the same as\n",
        "        the size of the cell output.\n",
        "    return_sequences: Boolean. Whether to return the last output.\n",
        "      in the output sequence, or the full sequence.\n",
        "    return_state: Boolean. Whether to return the last state\n",
        "      in addition to the output.\n",
        "    go_backwards: Boolean (default False).\n",
        "      If True, process the input sequence backwards and return the\n",
        "      reversed sequence.\n",
        "    stateful: Boolean (default False). If True, the last state\n",
        "      for each sample at index i in a batch will be used as initial\n",
        "      state for the sample of index i in the following batch.\n",
        "    input_shape: Use this argument to specify the shape of the\n",
        "      input when this layer is the first one in a model.\n",
        "  Call arguments:\n",
        "    inputs: A 5D tensor.\n",
        "    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n",
        "      a given timestep should be masked.\n",
        "    training: Python boolean indicating whether the layer should behave in\n",
        "      training mode or in inference mode. This argument is passed to the cell\n",
        "      when calling it. This is for use with cells that use dropout.\n",
        "    initial_state: List of initial state tensors to be passed to the first\n",
        "      call of the cell.\n",
        "    constants: List of constant tensors to be passed to the cell at each\n",
        "      timestep.\n",
        "  Input shape:\n",
        "    5D tensor with shape:\n",
        "    `(samples, timesteps, channels, rows, cols)`\n",
        "    if data_format='channels_first' or 5D tensor with shape:\n",
        "    `(samples, timesteps, rows, cols, channels)`\n",
        "    if data_format='channels_last'.\n",
        "  Output shape:\n",
        "    - If `return_state`: a list of tensors. The first tensor is\n",
        "      the output. The remaining tensors are the last states,\n",
        "      each 4D tensor with shape:\n",
        "      `(samples, filters, new_rows, new_cols)`\n",
        "      if data_format='channels_first'\n",
        "      or 4D tensor with shape:\n",
        "      `(samples, new_rows, new_cols, filters)`\n",
        "      if data_format='channels_last'.\n",
        "      `rows` and `cols` values might have changed due to padding.\n",
        "    - If `return_sequences`: 5D tensor with shape:\n",
        "      `(samples, timesteps, filters, new_rows, new_cols)`\n",
        "      if data_format='channels_first'\n",
        "      or 5D tensor with shape:\n",
        "      `(samples, timesteps, new_rows, new_cols, filters)`\n",
        "      if data_format='channels_last'.\n",
        "    - Else, 4D tensor with shape:\n",
        "      `(samples, filters, new_rows, new_cols)`\n",
        "      if data_format='channels_first'\n",
        "      or 4D tensor with shape:\n",
        "      `(samples, new_rows, new_cols, filters)`\n",
        "      if data_format='channels_last'.\n",
        "  Masking:\n",
        "    This layer supports masking for input data with a variable number\n",
        "    of timesteps.\n",
        "  Note on using statefulness in RNNs:\n",
        "    You can set RNN layers to be 'stateful', which means that the states\n",
        "    computed for the samples in one batch will be reused as initial states\n",
        "    for the samples in the next batch. This assumes a one-to-one mapping\n",
        "    between samples in different successive batches.\n",
        "    To enable statefulness:\n",
        "      - Specify `stateful=True` in the layer constructor.\n",
        "      - Specify a fixed batch size for your model, by passing\n",
        "         - If sequential model:\n",
        "            `batch_input_shape=(...)` to the first layer in your model.\n",
        "         - If functional model with 1 or more Input layers:\n",
        "            `batch_shape=(...)` to all the first layers in your model.\n",
        "            This is the expected shape of your inputs\n",
        "            *including the batch size*.\n",
        "            It should be a tuple of integers,\n",
        "            e.g. `(32, 10, 100, 100, 32)`.\n",
        "            Note that the number of rows and columns should be specified\n",
        "            too.\n",
        "      - Specify `shuffle=False` when calling fit().\n",
        "    To reset the states of your model, call `.reset_states()` on either\n",
        "    a specific layer, or on your entire model.\n",
        "  Note on specifying the initial state of RNNs:\n",
        "    You can specify the initial state of RNN layers symbolically by\n",
        "    calling them with the keyword argument `initial_state`. The value of\n",
        "    `initial_state` should be a tensor or list of tensors representing\n",
        "    the initial state of the RNN layer.\n",
        "    You can specify the initial state of RNN layers numerically by\n",
        "    calling `reset_states` with the keyword argument `states`. The value of\n",
        "    `states` should be a numpy array or list of numpy arrays representing\n",
        "    the initial state of the RNN layer.\n",
        "  Note on passing external constants to RNNs:\n",
        "    You can pass \"external\" constants to the cell using the `constants`\n",
        "    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n",
        "    requires that the `cell.call` method accepts the same keyword argument\n",
        "    `constants`. Such constants can be used to condition the cell\n",
        "    transformation on additional static inputs (not changing over time),\n",
        "    a.k.a. an attention mechanism.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               cell,\n",
        "               return_sequences=False,\n",
        "               return_state=False,\n",
        "               go_backwards=False,\n",
        "               stateful=False,\n",
        "               unroll=False,\n",
        "               **kwargs):\n",
        "    if unroll:\n",
        "      raise TypeError('Unrolling isn\\'t possible with '\n",
        "                      'convolutional RNNs.')\n",
        "    if isinstance(cell, (list, tuple)):\n",
        "      # The StackedConvRNN2DCells isn't implemented yet.\n",
        "      raise TypeError('It is not possible at the moment to'\n",
        "                      'stack convolutional cells.')\n",
        "    super(ConvRNN2D, self).__init__(cell,\n",
        "                                    return_sequences,\n",
        "                                    return_state,\n",
        "                                    go_backwards,\n",
        "                                    stateful,\n",
        "                                    unroll,\n",
        "                                    **kwargs)\n",
        "    self.input_spec = [InputSpec(ndim=5)]\n",
        "    self.states = None\n",
        "    self._num_constants = None\n",
        "\n",
        "  @tf_utils.shape_type_conversion\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if isinstance(input_shape, list):\n",
        "      input_shape = input_shape[0]\n",
        "\n",
        "    cell = self.cell\n",
        "    if cell.data_format == 'channels_first':\n",
        "      rows = input_shape[3]\n",
        "      cols = input_shape[4]\n",
        "    elif cell.data_format == 'channels_last':\n",
        "      rows = input_shape[2]\n",
        "      cols = input_shape[3]\n",
        "    rows = conv_utils.conv_output_length(rows,\n",
        "                                         cell.kernel_size[0],\n",
        "                                         padding=cell.padding,\n",
        "                                         stride=cell.strides[0],\n",
        "                                         dilation=cell.dilation_rate[0])\n",
        "    cols = conv_utils.conv_output_length(cols,\n",
        "                                         cell.kernel_size[1],\n",
        "                                         padding=cell.padding,\n",
        "                                         stride=cell.strides[1],\n",
        "                                         dilation=cell.dilation_rate[1])\n",
        "\n",
        "    if cell.data_format == 'channels_first':\n",
        "      output_shape = input_shape[:2] + (cell.filters, rows, cols)\n",
        "    elif cell.data_format == 'channels_last':\n",
        "      output_shape = input_shape[:2] + (rows, cols, cell.filters)\n",
        "\n",
        "    if not self.return_sequences:\n",
        "      output_shape = output_shape[:1] + output_shape[2:]\n",
        "\n",
        "    if self.return_state:\n",
        "      output_shape = [output_shape]\n",
        "      if cell.data_format == 'channels_first':\n",
        "        output_shape += [(input_shape[0], cell.filters, rows, cols)\n",
        "                         for _ in range(2)]\n",
        "      elif cell.data_format == 'channels_last':\n",
        "        output_shape += [(input_shape[0], rows, cols, cell.filters)\n",
        "                         for _ in range(2)]\n",
        "    return output_shape\n",
        "\n",
        "  @tf_utils.shape_type_conversion\n",
        "  def build(self, input_shape):\n",
        "    # Note input_shape will be list of shapes of initial states and\n",
        "    # constants if these are passed in __call__.\n",
        "    if self._num_constants is not None:\n",
        "      constants_shape = input_shape[-self._num_constants:]  # pylint: disable=E1130\n",
        "    else:\n",
        "      constants_shape = None\n",
        "\n",
        "    if isinstance(input_shape, list):\n",
        "      input_shape = input_shape[0]\n",
        "\n",
        "    batch_size = input_shape[0] if self.stateful else None\n",
        "    self.input_spec[0] = InputSpec(shape=(batch_size, None) + input_shape[2:5])\n",
        "\n",
        "    # allow cell (if layer) to build before we set or validate state_spec\n",
        "    if isinstance(self.cell, Layer):\n",
        "      step_input_shape = (input_shape[0],) + input_shape[2:]\n",
        "      if constants_shape is not None:\n",
        "        self.cell.build([step_input_shape] + constants_shape)\n",
        "      else:\n",
        "        self.cell.build(step_input_shape)\n",
        "\n",
        "    # set or validate state_spec\n",
        "    if hasattr(self.cell.state_size, '__len__'):\n",
        "      state_size = list(self.cell.state_size)\n",
        "    else:\n",
        "      state_size = [self.cell.state_size]\n",
        "\n",
        "    if self.state_spec is not None:\n",
        "      # initial_state was passed in call, check compatibility\n",
        "      if self.cell.data_format == 'channels_first':\n",
        "        ch_dim = 1\n",
        "      elif self.cell.data_format == 'channels_last':\n",
        "        ch_dim = 3\n",
        "      if [spec.shape[ch_dim] for spec in self.state_spec] != state_size:\n",
        "        raise ValueError(\n",
        "            'An initial_state was passed that is not compatible with '\n",
        "            '`cell.state_size`. Received `state_spec`={}; '\n",
        "            'However `cell.state_size` is '\n",
        "            '{}'.format([spec.shape for spec in self.state_spec],\n",
        "                        self.cell.state_size))\n",
        "    else:\n",
        "      if self.cell.data_format == 'channels_first':\n",
        "        self.state_spec = [InputSpec(shape=(None, dim, None, None))\n",
        "                           for dim in state_size]\n",
        "      elif self.cell.data_format == 'channels_last':\n",
        "        self.state_spec = [InputSpec(shape=(None, None, None, dim))\n",
        "                           for dim in state_size]\n",
        "    if self.stateful:\n",
        "      self.reset_states()\n",
        "    self.built = True\n",
        "\n",
        "  def get_initial_state(self, inputs):\n",
        "    # (samples, timesteps, rows, cols, filters)\n",
        "    initial_state = K.zeros_like(inputs)\n",
        "    # (samples, rows, cols, filters)\n",
        "    initial_state = K.sum(initial_state, axis=1)\n",
        "    shape = list(self.cell.kernel_shape)\n",
        "    shape[-1] = self.cell.filters\n",
        "    initial_state = self.cell.input_conv(initial_state,\n",
        "                                         array_ops.zeros(tuple(shape)),\n",
        "                                         padding=self.cell.padding)\n",
        "\n",
        "    if hasattr(self.cell.state_size, '__len__'):\n",
        "      return [initial_state for _ in self.cell.state_size]\n",
        "    else:\n",
        "      return [initial_state]\n",
        "\n",
        "  def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n",
        "    inputs, initial_state, constants = _standardize_args(\n",
        "        inputs, initial_state, constants, self._num_constants)\n",
        "\n",
        "    if initial_state is None and constants is None:\n",
        "      return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n",
        "\n",
        "    # If any of `initial_state` or `constants` are specified and are Keras\n",
        "    # tensors, then add them to the inputs and temporarily modify the\n",
        "    # input_spec to include them.\n",
        "\n",
        "    additional_inputs = []\n",
        "    additional_specs = []\n",
        "    if initial_state is not None:\n",
        "      kwargs['initial_state'] = initial_state\n",
        "      additional_inputs += initial_state\n",
        "      self.state_spec = []\n",
        "      for state in initial_state:\n",
        "        shape = K.int_shape(state)\n",
        "        self.state_spec.append(InputSpec(shape=shape))\n",
        "\n",
        "      additional_specs += self.state_spec\n",
        "    if constants is not None:\n",
        "      kwargs['constants'] = constants\n",
        "      additional_inputs += constants\n",
        "      self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n",
        "                             for constant in constants]\n",
        "      self._num_constants = len(constants)\n",
        "      additional_specs += self.constants_spec\n",
        "    # at this point additional_inputs cannot be empty\n",
        "    for tensor in additional_inputs:\n",
        "      if K.is_keras_tensor(tensor) != K.is_keras_tensor(additional_inputs[0]):\n",
        "        raise ValueError('The initial state or constants of an RNN'\n",
        "                         ' layer cannot be specified with a mix of'\n",
        "                         ' Keras tensors and non-Keras tensors')\n",
        "\n",
        "    if K.is_keras_tensor(additional_inputs[0]):\n",
        "      # Compute the full input spec, including state and constants\n",
        "      full_input = [inputs] + additional_inputs\n",
        "      full_input_spec = self.input_spec + additional_specs\n",
        "      # Perform the call with temporarily replaced input_spec\n",
        "      original_input_spec = self.input_spec\n",
        "      self.input_spec = full_input_spec\n",
        "      output = super(ConvRNN2D, self).__call__(full_input, **kwargs)\n",
        "      self.input_spec = original_input_spec\n",
        "      return output\n",
        "    else:\n",
        "      return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n",
        "\n",
        "  def call(self,\n",
        "           inputs,\n",
        "           mask=None,\n",
        "           training=None,\n",
        "           initial_state=None,\n",
        "           constants=None):\n",
        "    # note that the .build() method of subclasses MUST define\n",
        "    # self.input_spec and self.state_spec with complete input shapes.\n",
        "    if isinstance(inputs, list):\n",
        "      inputs = inputs[0]\n",
        "    if initial_state is not None:\n",
        "      pass\n",
        "    elif self.stateful:\n",
        "      initial_state = self.states\n",
        "    else:\n",
        "      initial_state = self.get_initial_state(inputs)\n",
        "\n",
        "    if isinstance(mask, list):\n",
        "      mask = mask[0]\n",
        "\n",
        "    if len(initial_state) != len(self.states):\n",
        "      raise ValueError('Layer has ' + str(len(self.states)) +\n",
        "                       ' states but was passed ' +\n",
        "                       str(len(initial_state)) +\n",
        "                       ' initial states.')\n",
        "    timesteps = K.int_shape(inputs)[1]\n",
        "\n",
        "    kwargs = {}\n",
        "    if generic_utils.has_arg(self.cell.call, 'training'):\n",
        "      kwargs['training'] = training\n",
        "\n",
        "    if constants:\n",
        "      if not generic_utils.has_arg(self.cell.call, 'constants'):\n",
        "        raise ValueError('RNN cell does not support constants')\n",
        "\n",
        "      def step(inputs, states):\n",
        "        constants = states[-self._num_constants:]\n",
        "        states = states[:-self._num_constants]\n",
        "        return self.cell.call(inputs, states, constants=constants,\n",
        "                              **kwargs)\n",
        "    else:\n",
        "      def step(inputs, states):\n",
        "        return self.cell.call(inputs, states, **kwargs)\n",
        "\n",
        "    last_output, outputs, states = K.rnn(step,\n",
        "                                         inputs,\n",
        "                                         initial_state,\n",
        "                                         constants=constants,\n",
        "                                         go_backwards=self.go_backwards,\n",
        "                                         mask=mask,\n",
        "                                         input_length=timesteps)\n",
        "    if self.stateful:\n",
        "      updates = []\n",
        "      for i in range(len(states)):\n",
        "        updates.append(K.update(self.states[i], states[i]))\n",
        "      self.add_update(updates)\n",
        "\n",
        "    if self.return_sequences:\n",
        "      output = outputs\n",
        "    else:\n",
        "      output = last_output\n",
        "\n",
        "    if self.return_state:\n",
        "      if not isinstance(states, (list, tuple)):\n",
        "        states = [states]\n",
        "      else:\n",
        "        states = list(states)\n",
        "      return [output] + states\n",
        "    else:\n",
        "      return output\n",
        "\n",
        "  def reset_states(self, states=None):\n",
        "    if not self.stateful:\n",
        "      raise AttributeError('Layer must be stateful.')\n",
        "    input_shape = self.input_spec[0].shape\n",
        "    state_shape = self.compute_output_shape(input_shape)\n",
        "    if self.return_state:\n",
        "      state_shape = state_shape[0]\n",
        "    if self.return_sequences:\n",
        "      state_shape = state_shape[:1].concatenate(state_shape[2:])\n",
        "    if None in state_shape:\n",
        "      raise ValueError('If a RNN is stateful, it needs to know '\n",
        "                       'its batch size. Specify the batch size '\n",
        "                       'of your input tensors: \\n'\n",
        "                       '- If using a Sequential model, '\n",
        "                       'specify the batch size by passing '\n",
        "                       'a `batch_input_shape` '\n",
        "                       'argument to your first layer.\\n'\n",
        "                       '- If using the functional API, specify '\n",
        "                       'the time dimension by passing a '\n",
        "                       '`batch_shape` argument to your Input layer.\\n'\n",
        "                       'The same thing goes for the number of rows and '\n",
        "                       'columns.')\n",
        "\n",
        "    # helper function\n",
        "    def get_tuple_shape(nb_channels):\n",
        "      result = list(state_shape)\n",
        "      if self.cell.data_format == 'channels_first':\n",
        "        result[1] = nb_channels\n",
        "      elif self.cell.data_format == 'channels_last':\n",
        "        result[3] = nb_channels\n",
        "      else:\n",
        "        raise KeyError\n",
        "      return tuple(result)\n",
        "\n",
        "    # initialize state if None\n",
        "    if self.states[0] is None:\n",
        "      if hasattr(self.cell.state_size, '__len__'):\n",
        "        self.states = [K.zeros(get_tuple_shape(dim))\n",
        "                       for dim in self.cell.state_size]\n",
        "      else:\n",
        "        self.states = [K.zeros(get_tuple_shape(self.cell.state_size))]\n",
        "    elif states is None:\n",
        "      if hasattr(self.cell.state_size, '__len__'):\n",
        "        for state, dim in zip(self.states, self.cell.state_size):\n",
        "          K.set_value(state, np.zeros(get_tuple_shape(dim)))\n",
        "      else:\n",
        "        K.set_value(self.states[0],\n",
        "                    np.zeros(get_tuple_shape(self.cell.state_size)))\n",
        "    else:\n",
        "      if not isinstance(states, (list, tuple)):\n",
        "        states = [states]\n",
        "      if len(states) != len(self.states):\n",
        "        raise ValueError('Layer ' + self.name + ' expects ' +\n",
        "                         str(len(self.states)) + ' states, ' +\n",
        "                         'but it received ' + str(len(states)) +\n",
        "                         ' state values. Input received: ' + str(states))\n",
        "      for index, (value, state) in enumerate(zip(states, self.states)):\n",
        "        if hasattr(self.cell.state_size, '__len__'):\n",
        "          dim = self.cell.state_size[index]\n",
        "        else:\n",
        "          dim = self.cell.state_size\n",
        "        if value.shape != get_tuple_shape(dim):\n",
        "          raise ValueError('State ' + str(index) +\n",
        "                           ' is incompatible with layer ' +\n",
        "                           self.name + ': expected shape=' +\n",
        "                           str(get_tuple_shape(dim)) +\n",
        "                           ', found shape=' + str(value.shape))\n",
        "        # TODO(anjalisridhar): consider batch calls to `set_value`.\n",
        "        K.set_value(state, value)\n",
        "\n",
        "\n",
        "class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n",
        "  \"\"\"Cell class for the ConvLSTM2D layer.\n",
        "  Arguments:\n",
        "    filters: Integer, the dimensionality of the output space\n",
        "      (i.e. the number of output filters in the convolution).\n",
        "    kernel_size: An integer or tuple/list of n integers, specifying the\n",
        "      dimensions of the convolution window.\n",
        "    strides: An integer or tuple/list of n integers,\n",
        "      specifying the strides of the convolution.\n",
        "      Specifying any stride value != 1 is incompatible with specifying\n",
        "      any `dilation_rate` value != 1.\n",
        "    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
        "    data_format: A string,\n",
        "      one of `channels_last` (default) or `channels_first`.\n",
        "      It defaults to the `image_data_format` value found in your\n",
        "      Keras config file at `~/.keras/keras.json`.\n",
        "      If you never set it, then it will be \"channels_last\".\n",
        "    dilation_rate: An integer or tuple/list of n integers, specifying\n",
        "      the dilation rate to use for dilated convolution.\n",
        "      Currently, specifying any `dilation_rate` value != 1 is\n",
        "      incompatible with specifying any `strides` value != 1.\n",
        "    activation: Activation function to use.\n",
        "      If you don't specify anything, no activation is applied\n",
        "      (ie. \"linear\" activation: `a(x) = x`).\n",
        "    recurrent_activation: Activation function to use\n",
        "      for the recurrent step.\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "      used for the linear transformation of the inputs.\n",
        "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "      weights matrix,\n",
        "      used for the linear transformation of the recurrent state.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    unit_forget_bias: Boolean.\n",
        "      If True, add 1 to the bias of the forget gate at initialization.\n",
        "      Use in combination with `bias_initializer=\"zeros\"`.\n",
        "      This is recommended in [Jozefowicz et al.]\n",
        "      (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
        "    kernel_regularizer: Regularizer function applied to\n",
        "      the `kernel` weights matrix.\n",
        "    recurrent_regularizer: Regularizer function applied to\n",
        "      the `recurrent_kernel` weights matrix.\n",
        "    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "    kernel_constraint: Constraint function applied to\n",
        "      the `kernel` weights matrix.\n",
        "    recurrent_constraint: Constraint function applied to\n",
        "      the `recurrent_kernel` weights matrix.\n",
        "    bias_constraint: Constraint function applied to the bias vector.\n",
        "    dropout: Float between 0 and 1.\n",
        "      Fraction of the units to drop for\n",
        "      the linear transformation of the inputs.\n",
        "    recurrent_dropout: Float between 0 and 1.\n",
        "      Fraction of the units to drop for\n",
        "      the linear transformation of the recurrent state.\n",
        "  Call arguments:\n",
        "    inputs: A 4D tensor.\n",
        "    states:  List of state tensors corresponding to the previous timestep.\n",
        "    training: Python boolean indicating whether the layer should behave in\n",
        "      training mode or in inference mode. Only relevant when `dropout` or\n",
        "      `recurrent_dropout` is used.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation='tanh',\n",
        "               recurrent_activation='hard_sigmoid',\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               recurrent_initializer='orthogonal',\n",
        "               bias_initializer='zeros',\n",
        "               unit_forget_bias=True,\n",
        "               kernel_regularizer=None,\n",
        "               recurrent_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               recurrent_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               dropout=0.,\n",
        "               recurrent_dropout=0.,\n",
        "               **kwargs):\n",
        "    super(ConvLSTM2DCell, self).__init__(**kwargs)\n",
        "    self.filters = filters\n",
        "    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n",
        "    self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n",
        "    self.padding = conv_utils.normalize_padding(padding)\n",
        "    self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "    self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n",
        "                                                    'dilation_rate')\n",
        "    self.activation = activations.get(activation)\n",
        "    self.recurrent_activation = activations.get(recurrent_activation)\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "    self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "    self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "    self.bias_initializer = initializers.get(bias_initializer)\n",
        "    self.unit_forget_bias = unit_forget_bias\n",
        "\n",
        "    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "    self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "    self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "    self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "    self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "    self.dropout = min(1., max(0., dropout))\n",
        "    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "    self.state_size = (self.filters, self.filters)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "\n",
        "    if self.data_format == 'channels_first':\n",
        "      channel_axis = 1\n",
        "    else:\n",
        "      channel_axis = -1\n",
        "    if input_shape[channel_axis] is None:\n",
        "      raise ValueError('The channel dimension of the inputs '\n",
        "                       'should be defined. Found `None`.')\n",
        "    input_dim = input_shape[channel_axis]\n",
        "    kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n",
        "    self.kernel_shape = kernel_shape\n",
        "    recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n",
        "\n",
        "    self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='kernel',\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "    self.recurrent_kernel = self.add_weight(\n",
        "        shape=recurrent_kernel_shape,\n",
        "        initializer=self.recurrent_initializer,\n",
        "        name='recurrent_kernel',\n",
        "        regularizer=self.recurrent_regularizer,\n",
        "        constraint=self.recurrent_constraint)\n",
        "\n",
        "    if self.use_bias:\n",
        "      if self.unit_forget_bias:\n",
        "\n",
        "        def bias_initializer(_, *args, **kwargs):\n",
        "          return K.concatenate([\n",
        "              self.bias_initializer((self.filters,), *args, **kwargs),\n",
        "              initializers.Ones()((self.filters,), *args, **kwargs),\n",
        "              self.bias_initializer((self.filters * 2,), *args, **kwargs),\n",
        "          ])\n",
        "      else:\n",
        "        bias_initializer = self.bias_initializer\n",
        "      self.bias = self.add_weight(\n",
        "          shape=(self.filters * 4,),\n",
        "          name='bias',\n",
        "          initializer=bias_initializer,\n",
        "          regularizer=self.bias_regularizer,\n",
        "          constraint=self.bias_constraint)\n",
        "    else:\n",
        "      self.bias = None\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, states, training=None):\n",
        "    h_tm1 = states[0]  # previous memory state\n",
        "    c_tm1 = states[1]  # previous carry state\n",
        "\n",
        "    # dropout matrices for input units\n",
        "    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n",
        "    # dropout matrices for recurrent units\n",
        "    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
        "        h_tm1, training, count=4)\n",
        "\n",
        "    if 0 < self.dropout < 1.:\n",
        "      inputs_i = inputs * dp_mask[0]\n",
        "      inputs_f = inputs * dp_mask[1]\n",
        "      inputs_c = inputs * dp_mask[2]\n",
        "      inputs_o = inputs * dp_mask[3]\n",
        "    else:\n",
        "      inputs_i = inputs\n",
        "      inputs_f = inputs\n",
        "      inputs_c = inputs\n",
        "      inputs_o = inputs\n",
        "\n",
        "    if 0 < self.recurrent_dropout < 1.:\n",
        "      h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
        "      h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
        "      h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
        "      h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
        "    else:\n",
        "      h_tm1_i = h_tm1\n",
        "      h_tm1_f = h_tm1\n",
        "      h_tm1_c = h_tm1\n",
        "      h_tm1_o = h_tm1\n",
        "\n",
        "    (kernel_i, kernel_f,\n",
        "     kernel_c, kernel_o) = array_ops.split(self.kernel, 4, axis=3)\n",
        "    (recurrent_kernel_i,\n",
        "     recurrent_kernel_f,\n",
        "     recurrent_kernel_c,\n",
        "     recurrent_kernel_o) = array_ops.split(self.recurrent_kernel, 4, axis=3)\n",
        "\n",
        "    if self.use_bias:\n",
        "      bias_i, bias_f, bias_c, bias_o = array_ops.split(self.bias, 4)\n",
        "    else:\n",
        "      bias_i, bias_f, bias_c, bias_o = None, None, None, None\n",
        "\n",
        "    x_i = self.input_conv(inputs_i, kernel_i, bias_i, padding=self.padding)\n",
        "    x_f = self.input_conv(inputs_f, kernel_f, bias_f, padding=self.padding)\n",
        "    x_c = self.input_conv(inputs_c, kernel_c, bias_c, padding=self.padding)\n",
        "    x_o = self.input_conv(inputs_o, kernel_o, bias_o, padding=self.padding)\n",
        "    h_i = self.recurrent_conv(h_tm1_i, recurrent_kernel_i)\n",
        "    h_f = self.recurrent_conv(h_tm1_f, recurrent_kernel_f)\n",
        "    h_c = self.recurrent_conv(h_tm1_c, recurrent_kernel_c)\n",
        "    h_o = self.recurrent_conv(h_tm1_o, recurrent_kernel_o)\n",
        "\n",
        "    i = self.recurrent_activation(x_i + h_i)\n",
        "    f = self.recurrent_activation(x_f + h_f)\n",
        "    c = f * c_tm1 + i * self.activation(x_c + h_c)\n",
        "    o = self.recurrent_activation(x_o + h_o)\n",
        "    h = o * self.activation(c)\n",
        "    return h, [h, c]\n",
        "\n",
        "  def input_conv(self, x, w, b=None, padding='valid'):\n",
        "    conv_out = K.conv2d(x, w, strides=self.strides,\n",
        "                        padding=padding,\n",
        "                        data_format=self.data_format,\n",
        "                        dilation_rate=self.dilation_rate)\n",
        "    if b is not None:\n",
        "      conv_out = K.bias_add(conv_out, b,\n",
        "                            data_format=self.data_format)\n",
        "    return conv_out\n",
        "\n",
        "  def recurrent_conv(self, x, w):\n",
        "    conv_out = K.conv2d(x, w, strides=(1, 1),\n",
        "                        padding='same',\n",
        "                        data_format=self.data_format)\n",
        "    return conv_out\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {'filters': self.filters,\n",
        "              'kernel_size': self.kernel_size,\n",
        "              'strides': self.strides,\n",
        "              'padding': self.padding,\n",
        "              'data_format': self.data_format,\n",
        "              'dilation_rate': self.dilation_rate,\n",
        "              'activation': activations.serialize(self.activation),\n",
        "              'recurrent_activation': activations.serialize(\n",
        "                  self.recurrent_activation),\n",
        "              'use_bias': self.use_bias,\n",
        "              'kernel_initializer': initializers.serialize(\n",
        "                  self.kernel_initializer),\n",
        "              'recurrent_initializer': initializers.serialize(\n",
        "                  self.recurrent_initializer),\n",
        "              'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "              'unit_forget_bias': self.unit_forget_bias,\n",
        "              'kernel_regularizer': regularizers.serialize(\n",
        "                  self.kernel_regularizer),\n",
        "              'recurrent_regularizer': regularizers.serialize(\n",
        "                  self.recurrent_regularizer),\n",
        "              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "              'kernel_constraint': constraints.serialize(\n",
        "                  self.kernel_constraint),\n",
        "              'recurrent_constraint': constraints.serialize(\n",
        "                  self.recurrent_constraint),\n",
        "              'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "              'dropout': self.dropout,\n",
        "              'recurrent_dropout': self.recurrent_dropout}\n",
        "    base_config = super(ConvLSTM2DCell, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPa4mqOMtOWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPSILON = 0.00001\n",
        "def tensor_layer_norm(x, state_name):\n",
        "    x_shape = x.get_shape()\n",
        "    dims = x_shape.ndims\n",
        "    params_shape = x_shape[-1:]\n",
        "    if dims == 4:\n",
        "        m, v = tf.nn.moments(x, [1,2,3], keep_dims=True)\n",
        "    elif dims == 5:\n",
        "        m, v = tf.nn.moments(x, [1,2,3,4], keep_dims=True)\n",
        "    else:\n",
        "        raise ValueError('input tensor for layer normalization must be rank 4 or 5.')\n",
        "    b = tf.get_variable(state_name+'b',initializer=tf.zeros(params_shape))\n",
        "    s = tf.get_variable(state_name+'s',initializer=tf.ones(params_shape))\n",
        "    x_tln = tf.nn.batch_normalization(x, m, v, b, s, EPSILON)\n",
        "    return x_tln"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ30NUb7772z",
        "colab_type": "text"
      },
      "source": [
        "# CausalLSTMCell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3km7gr2mAlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalLSTMCell(DropoutRNNCellMixin, Layer):\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               forget_bias=1.0,\n",
        "               tln=False,\n",
        "               initializer=0.001,\n",
        "               layer_name=\"causal_lstm\",\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation='tanh',\n",
        "               recurrent_activation='hard_sigmoid',\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               recurrent_initializer='orthogonal',\n",
        "               bias_initializer='zeros',\n",
        "               unit_forget_bias=True,\n",
        "               kernel_regularizer=None,\n",
        "               recurrent_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               recurrent_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               dropout=0.,\n",
        "               recurrent_dropout=0.,\n",
        "               **kwargs):\n",
        "        super(CausalLSTMCell, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.num_hidden = filters\n",
        "        self.filter_size = kernel_size[0]\n",
        "        self.layer_norm = tln\n",
        "        self._forget_bias = forget_bias\n",
        "        self.initializer = tf.random_uniform_initializer(-initializer,initializer)\n",
        "        self.layer_name = layer_name\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n",
        "                                                        'dilation_rate')\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.unit_forget_bias = unit_forget_bias\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = (self.filters, self.filters)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                        'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n",
        "        self.kernel_shape = kernel_shape\n",
        "        recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                    initializer=self.kernel_initializer,\n",
        "                                    name='kernel',\n",
        "                                    regularizer=self.kernel_regularizer,\n",
        "                                    constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=recurrent_kernel_shape,\n",
        "            initializer=self.recurrent_initializer,\n",
        "            name='recurrent_kernel',\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.unit_forget_bias:\n",
        "\n",
        "                def bias_initializer(_, *args, **kwargs):\n",
        "                    return K.concatenate([\n",
        "                        self.bias_initializer((self.filters,), *args, **kwargs),\n",
        "                        initializers.Ones()((self.filters,), *args, **kwargs),\n",
        "                        self.bias_initializer((self.filters * 2,), *args, **kwargs),\n",
        "                     ])\n",
        "            else:\n",
        "                bias_initializer = self.bias_initializer\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(self.filters * 4,),\n",
        "                name='bias',\n",
        "                initializer=bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x, states):\n",
        "        h = states[0]\n",
        "        c = states[1]\n",
        "        m = states[2]\n",
        "        self.batch = x.shape[0]\n",
        "        self.height = x.shape[2]\n",
        "        self.width = x.shape[3]\n",
        "        if h is None:\n",
        "            h = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden],\n",
        "                         dtype=tf.float32)\n",
        "        if c is None:\n",
        "            c = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden],\n",
        "                         dtype=tf.float32)\n",
        "        if m is None:\n",
        "            m = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden_in],\n",
        "                         dtype=tf.float32)\n",
        "\n",
        "        #with tf.variable_scope(self.layer_name):\n",
        "        h_cc = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*4,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='temporal_state_transition')(h)\n",
        "        c_cc = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*3,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='temporal_memory_transition')(c)\n",
        "        m_cc = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*3,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='spatial_memory_transition')(m)\n",
        "        if self.layer_norm:\n",
        "            h_cc = tensor_layer_norm(h_cc, 'h2c')\n",
        "            c_cc = tensor_layer_norm(c_cc, 'c2c')\n",
        "            m_cc = tensor_layer_norm(m_cc, 'm2m')\n",
        "\n",
        "        i_h, g_h, f_h, o_h = tf.split(h_cc, 4, 3)\n",
        "        i_c, g_c, f_c = tf.split(c_cc, 3, 3)\n",
        "        i_m, f_m, m_m = tf.split(m_cc, 3, 3)\n",
        "\n",
        "        if x is None:\n",
        "            i = tf.sigmoid(i_h + i_c)\n",
        "            f = tf.sigmoid(f_h + f_c + self._forget_bias)\n",
        "            g = tf.tanh(g_h + g_c)\n",
        "        else:\n",
        "            x_cc = tf.keras.layers.Conv2D(\n",
        "                self.num_hidden*7,\n",
        "                self.filter_size, 1, padding=self.padding,\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='input_to_state')(x)\n",
        "            if self.layer_norm:\n",
        "                x_cc = tensor_layer_norm(x_cc, 'x2c')\n",
        "\n",
        "            i_x, g_x, f_x, o_x, i_x_, g_x_, f_x_ = tf.split(x_cc, 7, 3)\n",
        "\n",
        "            i = tf.sigmoid(i_x + i_h + i_c)\n",
        "            f = tf.sigmoid(f_x + f_h + f_c + self._forget_bias)\n",
        "            g = tf.tanh(g_x + g_h + g_c)\n",
        "\n",
        "        c_new = f * c + i * g\n",
        "\n",
        "        c2m = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*4,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='c2m')(c_new)\n",
        "        if self.layer_norm:\n",
        "            c2m = tensor_layer_norm(c2m, 'c2m')\n",
        "\n",
        "        i_c, g_c, f_c, o_c = tf.split(c2m, 4, 3)\n",
        "\n",
        "        if x is None:\n",
        "            ii = tf.sigmoid(i_c + i_m)\n",
        "            ff = tf.sigmoid(f_c + f_m + self._forget_bias)\n",
        "            gg = tf.tanh(g_c)\n",
        "        else:\n",
        "            ii = tf.sigmoid(i_c + i_x_ + i_m)\n",
        "            ff = tf.sigmoid(f_c + f_x_ + f_m + self._forget_bias)\n",
        "            gg = tf.tanh(g_c + g_x_)\n",
        "\n",
        "        m_new = ff * tf.tanh(m_m) + ii * gg\n",
        "\n",
        "        o_m = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='m_to_o')(m_new)\n",
        "        if self.layer_norm:\n",
        "            o_m = tensor_layer_norm(o_m, 'm2o')\n",
        "\n",
        "        if x is None:\n",
        "            o = tf.tanh(o_h + o_c + o_m)\n",
        "        else:\n",
        "            o = tf.tanh(o_x + o_h + o_c + o_m)\n",
        "\n",
        "        cell = tf.concat([c_new, m_new],-1)\n",
        "        cell = tf.keras.layers.Conv2D(self.num_hidden, 1, 1,\n",
        "                                padding=self.padding, name='memory_reduce')(cell)\n",
        "        print(cell)\n",
        "        h_new = o * tf.tanh(cell)\n",
        "\n",
        "        #return h_new, c_new, m_new\n",
        "        return h_new, [h_new, c_new, m_new]\n",
        "\n",
        "    def input_conv(self, x, w, b=None, padding='valid'):\n",
        "        conv_out = K.conv2d(x, w, strides=self.strides,\n",
        "                            padding=padding,\n",
        "                            data_format=self.data_format,\n",
        "                            dilation_rate=self.dilation_rate)\n",
        "        if b is not None:\n",
        "            conv_out = K.bias_add(conv_out, b,\n",
        "                                data_format=self.data_format)\n",
        "        return conv_out\n",
        "\n",
        "    def recurrent_conv(self, x, w):\n",
        "        conv_out = K.conv2d(x, w, strides=(1, 1),\n",
        "                            padding='same',\n",
        "                            data_format=self.data_format)\n",
        "        return conv_out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'filters': self.filters,\n",
        "              'kernel_size': self.kernel_size,\n",
        "              'strides': self.strides,\n",
        "              'padding': self.padding,\n",
        "              'data_format': self.data_format,\n",
        "              'dilation_rate': self.dilation_rate,\n",
        "              'activation': activations.serialize(self.activation),\n",
        "              'recurrent_activation': activations.serialize(\n",
        "                  self.recurrent_activation),\n",
        "              'use_bias': self.use_bias,\n",
        "              'kernel_initializer': initializers.serialize(\n",
        "                  self.kernel_initializer),\n",
        "              'recurrent_initializer': initializers.serialize(\n",
        "                  self.recurrent_initializer),\n",
        "              'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "              'unit_forget_bias': self.unit_forget_bias,\n",
        "              'kernel_regularizer': regularizers.serialize(\n",
        "                  self.kernel_regularizer),\n",
        "              'recurrent_regularizer': regularizers.serialize(\n",
        "                  self.recurrent_regularizer),\n",
        "              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "              'kernel_constraint': constraints.serialize(\n",
        "                  self.kernel_constraint),\n",
        "              'recurrent_constraint': constraints.serialize(\n",
        "                  self.recurrent_constraint),\n",
        "              'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "              'dropout': self.dropout,\n",
        "              'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(ConvLSTM2DCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7kJYXNu738-",
        "colab_type": "text"
      },
      "source": [
        "# CausalLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKFr0R1ULjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@keras_export('keras.layers.CausalLSTM')\n",
        "class CausalLSTM(ConvRNN2D):\n",
        "\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               layer_name='causal_lstm',\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation='tanh',\n",
        "               recurrent_activation='hard_sigmoid',\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               recurrent_initializer='orthogonal',\n",
        "               bias_initializer='zeros',\n",
        "               unit_forget_bias=True,\n",
        "               kernel_regularizer=None,\n",
        "               recurrent_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               recurrent_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               return_sequences=False,\n",
        "               go_backwards=False,\n",
        "               stateful=False,\n",
        "               dropout=0.,\n",
        "               recurrent_dropout=0.,\n",
        "               **kwargs):\n",
        "        num_hidden = filters\n",
        "        filter_size = kernel_size[0]\n",
        "        cell = CausalLSTMCell(\n",
        "                        filters=filters,\n",
        "                          kernel_size=kernel_size,\n",
        "                          strides=strides,\n",
        "                          padding=padding,\n",
        "                          data_format=data_format,\n",
        "                          dilation_rate=dilation_rate,\n",
        "                          activation=activation,\n",
        "                          recurrent_activation=recurrent_activation,\n",
        "                          use_bias=use_bias,\n",
        "                          kernel_initializer=kernel_initializer,\n",
        "                          layer_name=layer_name,\n",
        "                          recurrent_initializer=recurrent_initializer,\n",
        "                          bias_initializer=bias_initializer,\n",
        "                          unit_forget_bias=unit_forget_bias,\n",
        "                          kernel_regularizer=kernel_regularizer,\n",
        "                          recurrent_regularizer=recurrent_regularizer,\n",
        "                          bias_regularizer=bias_regularizer,\n",
        "                          kernel_constraint=kernel_constraint,\n",
        "                          recurrent_constraint=recurrent_constraint,\n",
        "                          bias_constraint=bias_constraint,\n",
        "                          dropout=dropout,\n",
        "                          recurrent_dropout=recurrent_dropout,\n",
        "                          dtype=kwargs.get('dtype')\n",
        "        )\n",
        "        super(CausalLSTM, self).__init__(cell,\n",
        "                                         return_sequences=return_sequences,\n",
        "                                         go_backwards=go_backwards,\n",
        "                                         stateful=stateful,\n",
        "                                         **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        \n",
        "        def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "            return super (CausalLSTM, self).call(inputs,\n",
        "                                                 mask=mask,\n",
        "                                                 training=training,\n",
        "                                                 initial_state=initial_state)\n",
        "\n",
        "        @property\n",
        "        def filters(self):\n",
        "            return self.cell.filters\n",
        "\n",
        "        @property\n",
        "        def kernel_size(self):\n",
        "            return self.cell.kernel_size\n",
        "\n",
        "        @property\n",
        "        def strides(self):\n",
        "            return self.cell.strides\n",
        "\n",
        "        @property\n",
        "        def padding(self):\n",
        "            return self.cell.padding\n",
        "\n",
        "        @property\n",
        "        def data_format(self):\n",
        "            return self.cell.data_format\n",
        "\n",
        "        @property\n",
        "        def dilation_rate(self):\n",
        "            return self.cell.dilation_rate\n",
        "\n",
        "        @property\n",
        "        def activation(self):\n",
        "            return self.cell.activation\n",
        "\n",
        "        @property\n",
        "        def recurrent_activation(self):\n",
        "            return self.cell.recurrent_activation\n",
        "\n",
        "        @property\n",
        "        def use_bias(self):\n",
        "            return self.cell.use_bias\n",
        "\n",
        "        @property\n",
        "        def kernel_initializer(self):\n",
        "            return self.cell.kernel_initializer\n",
        "\n",
        "        @property\n",
        "        def recurrent_initializer(self):\n",
        "            return self.cell.recurrent_initializer\n",
        "\n",
        "        @property\n",
        "        def bias_initializer(self):\n",
        "            return self.cell.bias_initializer\n",
        "\n",
        "        @property\n",
        "        def unit_forget_bias(self):\n",
        "            return self.cell.unit_forget_bias\n",
        "\n",
        "        @property\n",
        "        def kernel_regularizer(self):\n",
        "            return self.cell.kernel_regularizer\n",
        "\n",
        "        @property\n",
        "        def recurrent_regularizer(self):\n",
        "            return self.cell.recurrent_regularizer\n",
        "\n",
        "        @property\n",
        "        def bias_regularizer(self):\n",
        "            return self.cell.bias_regularizer\n",
        "\n",
        "        @property\n",
        "        def kernel_constraint(self):\n",
        "            return self.cell.kernel_constraint\n",
        "\n",
        "        @property\n",
        "        def recurrent_constraint(self):\n",
        "            return self.cell.recurrent_constraint\n",
        "\n",
        "        @property\n",
        "        def bias_constraint(self):\n",
        "            return self.cell.bias_constraint\n",
        "\n",
        "        @property\n",
        "        def dropout(self):\n",
        "            return self.cell.dropout\n",
        "\n",
        "        @property\n",
        "        def recurrent_dropout(self):\n",
        "            return self.cell.recurrent_dropout\n",
        "\n",
        "        def get_config(self):\n",
        "            config = {'filters': self.filters,\n",
        "                    'kernel_size': self.kernel_size,\n",
        "                    'strides': self.strides,\n",
        "                    'padding': self.padding,\n",
        "                    'data_format': self.data_format,\n",
        "                    'dilation_rate': self.dilation_rate,\n",
        "                    'activation': activations.serialize(self.activation),\n",
        "                    'recurrent_activation':\n",
        "                        activations.serialize(self.recurrent_activation),\n",
        "                    'use_bias': self.use_bias,\n",
        "                    'kernel_initializer':\n",
        "                        initializers.serialize(self.kernel_initializer),\n",
        "                    'recurrent_initializer':\n",
        "                        initializers.serialize(self.recurrent_initializer),\n",
        "                    'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                    'unit_forget_bias': self.unit_forget_bias,\n",
        "                    'kernel_regularizer':\n",
        "                        regularizers.serialize(self.kernel_regularizer),\n",
        "                    'recurrent_regularizer':\n",
        "                        regularizers.serialize(self.recurrent_regularizer),\n",
        "                    'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                    'activity_regularizer':\n",
        "                        regularizers.serialize(self.activity_regularizer),\n",
        "                    'kernel_constraint':\n",
        "                        constraints.serialize(self.kernel_constraint),\n",
        "                    'recurrent_constraint':\n",
        "                        constraints.serialize(self.recurrent_constraint),\n",
        "                    'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                    'dropout': self.dropout,\n",
        "                    'recurrent_dropout': self.recurrent_dropout}\n",
        "            base_config = super(ConvLSTM2D, self).get_config()\n",
        "            del base_config['cell']\n",
        "            return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "        @classmethod\n",
        "        def from_config(cls, config):\n",
        "            return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqsB6ngL71Ac",
        "colab_type": "text"
      },
      "source": [
        "# GHU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjyncnnQNRTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GHU(keras.layers.Layer):\n",
        "    def __init__(self, layer_name, filter_size, num_features, tln=False,\n",
        "                 initializer=0.001, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \"\"\"Initialize the Gradient Highway Unit.\n",
        "        \"\"\"\n",
        "        self.layer_name = layer_name\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self.layer_norm = tln\n",
        "        if initializer == -1:\n",
        "            self.initializer = None\n",
        "        else:\n",
        "            self.initializer = tf.random_uniform_initializer(-initializer,initializer)\n",
        "\n",
        "    def init_state(self, inputs, num_features):\n",
        "        dims = inputs.get_shape().ndims\n",
        "        if dims == 4:\n",
        "            batch = inputs.get_shape()[0]\n",
        "            height = inputs.get_shape()[1]\n",
        "            width = inputs.get_shape()[2]\n",
        "        else:\n",
        "            raise ValueError('input tensor should be rank 4.')\n",
        "        return tf.zeros([batch, height, width, num_features], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x, z):\n",
        "        if z is None:\n",
        "            z = self.init_state(x, self.num_features)\n",
        "        with tf.variable_scope(self.layer_name):\n",
        "            z_concat = tf.keras.layers.Conv2D(\n",
        "                z, self.num_features*2,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='state_to_state')\n",
        "            if self.layer_norm:\n",
        "                z_concat = tensor_layer_norm(z_concat, 'state_to_state')\n",
        "\n",
        "            x_concat = tf.keras.layers.Conv2D(\n",
        "                x, self.num_features*2,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='input_to_state')\n",
        "            if self.layer_norm:\n",
        "                x_concat = tensor_layer_norm(x_concat, 'input_to_state')\n",
        "\n",
        "            gates = tf.add(x_concat, z_concat)\n",
        "            p, u = tf.split(gates, 2, 3)\n",
        "            p = tf.nn.tanh(p)\n",
        "            u = tf.nn.sigmoid(u)\n",
        "            z_new = u * p + (1-u) * z\n",
        "            return z_new\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgjmvA5reMx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Input\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw4dz7d-erHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = np.load('anom_train.npz')\n",
        "train_set = train_set['input_raw_data']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVDywNbd5Lny",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAPpiZBBY1w-",
        "colab_type": "code",
        "outputId": "7149c327-5708-4dd3-8aa6-a0f7634fc26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_set = train_set.reshape([10000,20,64,64,1])\n",
        "train_set.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 20, 64, 64, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOwzdJL8E8rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = np.swapaxes(train_set,1,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d13Cf3soFY_w",
        "colab_type": "code",
        "outputId": "a288fe3d-9900-4ed8-d2e7-1ff93dea52ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_set.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 10000, 64, 64, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrcCRiniEpMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(train_set[2,19,...,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo6CulzIXiut",
        "colab_type": "code",
        "outputId": "49ec89b4-d281-4a5a-9808-f1ce30459670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from keras.models import Sequential\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "          CausalLSTM(filters=1, #64\n",
        "                  kernel_size=(3,3),\n",
        "                  input_shape=(None, 64, 64, 1),\n",
        "                  padding='same', \n",
        "                  return_sequences=True)                          \n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
        "model.fit(train_set[:10], train_set[:10], batch_size=10,\n",
        "        epochs=300, validation_split=0.05)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1bab02b4b870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                   return_sequences=True)                          \n\u001b[0m\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adadelta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    176\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d5fcfca4b7d6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvRNN2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in converted code:\n\n    <ipython-input-16-d5fcfca4b7d6>:337 step  *\n        return self.cell.call(inputs, states, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:4035 rnn\n        input_time_zero, tuple(initial_states) + tuple(constants))\n    <ipython-input-18-14c7da550ba6>:114 call  *\n        m = states[2]\n\n    IndexError: tuple index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Cl1OIyGQQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}