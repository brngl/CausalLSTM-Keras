{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Causal LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brngl/CausalLSTM-Keras/blob/master/Keras_Causal_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "derY5ibBCqEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqssfFSu8zrc",
        "colab_type": "text"
      },
      "source": [
        "# Inizio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFy6MXDenJJO",
        "colab_type": "code",
        "outputId": "21a5f60c-0e77-4bd0-a0ba-8ac9efc9c3e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.keras import activations\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras import constraints\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras.engine.base_layer import Layer\n",
        "from tensorflow.python.keras.engine.input_spec import InputSpec\n",
        "from tensorflow.python.keras.layers.recurrent import _standardize_args\n",
        "from tensorflow.python.keras.layers.recurrent import DropoutRNNCellMixin\n",
        "from tensorflow.python.keras.layers.recurrent import RNN\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo3FU2-mCwX8",
        "colab_type": "text"
      },
      "source": [
        "#Versione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-PD63lBCHgE",
        "colab_type": "code",
        "outputId": "15a8ca97-5156-4995-e701-e40013691b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5xeanN0fS3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvRNN2D(RNN):\n",
        "  \"\"\"Base class for convolutional-recurrent layers.\n",
        "  Arguments:\n",
        "    cell: A RNN cell instance. A RNN cell is a class that has:\n",
        "      - a `call(input_at_t, states_at_t)` method, returning\n",
        "        `(output_at_t, states_at_t_plus_1)`. The call method of the\n",
        "        cell can also take the optional argument `constants`, see\n",
        "        section \"Note on passing external constants\" below.\n",
        "      - a `state_size` attribute. This can be a single integer\n",
        "        (single state) in which case it is\n",
        "        the number of channels of the recurrent state\n",
        "        (which should be the same as the number of channels of the cell\n",
        "        output). This can also be a list/tuple of integers\n",
        "        (one size per state). In this case, the first entry\n",
        "        (`state_size[0]`) should be the same as\n",
        "        the size of the cell output.\n",
        "    return_sequences: Boolean. Whether to return the last output.\n",
        "      in the output sequence, or the full sequence.\n",
        "    return_state: Boolean. Whether to return the last state\n",
        "      in addition to the output.\n",
        "    go_backwards: Boolean (default False).\n",
        "      If True, process the input sequence backwards and return the\n",
        "      reversed sequence.\n",
        "    stateful: Boolean (default False). If True, the last state\n",
        "      for each sample at index i in a batch will be used as initial\n",
        "      state for the sample of index i in the following batch.\n",
        "    input_shape: Use this argument to specify the shape of the\n",
        "      input when this layer is the first one in a model.\n",
        "  Call arguments:\n",
        "    inputs: A 5D tensor.\n",
        "    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n",
        "      a given timestep should be masked.\n",
        "    training: Python boolean indicating whether the layer should behave in\n",
        "      training mode or in inference mode. This argument is passed to the cell\n",
        "      when calling it. This is for use with cells that use dropout.\n",
        "    initial_state: List of initial state tensors to be passed to the first\n",
        "      call of the cell.\n",
        "    constants: List of constant tensors to be passed to the cell at each\n",
        "      timestep.\n",
        "  Input shape:\n",
        "    5D tensor with shape:\n",
        "    `(samples, timesteps, channels, rows, cols)`\n",
        "    if data_format='channels_first' or 5D tensor with shape:\n",
        "    `(samples, timesteps, rows, cols, channels)`\n",
        "    if data_format='channels_last'.\n",
        "  Output shape:\n",
        "    - If `return_state`: a list of tensors. The first tensor is\n",
        "      the output. The remaining tensors are the last states,\n",
        "      each 4D tensor with shape:\n",
        "      `(samples, filters, new_rows, new_cols)`\n",
        "      if data_format='channels_first'\n",
        "      or 4D tensor with shape:\n",
        "      `(samples, new_rows, new_cols, filters)`\n",
        "      if data_format='channels_last'.\n",
        "      `rows` and `cols` values might have changed due to padding.\n",
        "    - If `return_sequences`: 5D tensor with shape:\n",
        "      `(samples, timesteps, filters, new_rows, new_cols)`\n",
        "      if data_format='channels_first'\n",
        "      or 5D tensor with shape:\n",
        "      `(samples, timesteps, new_rows, new_cols, filters)`\n",
        "      if data_format='channels_last'.\n",
        "    - Else, 4D tensor with shape:\n",
        "      `(samples, filters, new_rows, new_cols)`\n",
        "      if data_format='channels_first'\n",
        "      or 4D tensor with shape:\n",
        "      `(samples, new_rows, new_cols, filters)`\n",
        "      if data_format='channels_last'.\n",
        "  Masking:\n",
        "    This layer supports masking for input data with a variable number\n",
        "    of timesteps.\n",
        "  Note on using statefulness in RNNs:\n",
        "    You can set RNN layers to be 'stateful', which means that the states\n",
        "    computed for the samples in one batch will be reused as initial states\n",
        "    for the samples in the next batch. This assumes a one-to-one mapping\n",
        "    between samples in different successive batches.\n",
        "    To enable statefulness:\n",
        "      - Specify `stateful=True` in the layer constructor.\n",
        "      - Specify a fixed batch size for your model, by passing\n",
        "         - If sequential model:\n",
        "            `batch_input_shape=(...)` to the first layer in your model.\n",
        "         - If functional model with 1 or more Input layers:\n",
        "            `batch_shape=(...)` to all the first layers in your model.\n",
        "            This is the expected shape of your inputs\n",
        "            *including the batch size*.\n",
        "            It should be a tuple of integers,\n",
        "            e.g. `(32, 10, 100, 100, 32)`.\n",
        "            Note that the number of rows and columns should be specified\n",
        "            too.\n",
        "      - Specify `shuffle=False` when calling fit().\n",
        "    To reset the states of your model, call `.reset_states()` on either\n",
        "    a specific layer, or on your entire model.\n",
        "  Note on specifying the initial state of RNNs:\n",
        "    You can specify the initial state of RNN layers symbolically by\n",
        "    calling them with the keyword argument `initial_state`. The value of\n",
        "    `initial_state` should be a tensor or list of tensors representing\n",
        "    the initial state of the RNN layer.\n",
        "    You can specify the initial state of RNN layers numerically by\n",
        "    calling `reset_states` with the keyword argument `states`. The value of\n",
        "    `states` should be a numpy array or list of numpy arrays representing\n",
        "    the initial state of the RNN layer.\n",
        "  Note on passing external constants to RNNs:\n",
        "    You can pass \"external\" constants to the cell using the `constants`\n",
        "    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n",
        "    requires that the `cell.call` method accepts the same keyword argument\n",
        "    `constants`. Such constants can be used to condition the cell\n",
        "    transformation on additional static inputs (not changing over time),\n",
        "    a.k.a. an attention mechanism.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               cell,\n",
        "               return_sequences=False,\n",
        "               return_state=False,\n",
        "               go_backwards=False,\n",
        "               stateful=False,\n",
        "               unroll=False,\n",
        "               **kwargs):\n",
        "    if unroll:\n",
        "      raise TypeError('Unrolling isn\\'t possible with '\n",
        "                      'convolutional RNNs.')\n",
        "    if isinstance(cell, (list, tuple)):\n",
        "      # The StackedConvRNN2DCells isn't implemented yet.\n",
        "      raise TypeError('It is not possible at the moment to'\n",
        "                      'stack convolutional cells.')\n",
        "    super(ConvRNN2D, self).__init__(cell,\n",
        "                                    return_sequences,\n",
        "                                    return_state,\n",
        "                                    go_backwards,\n",
        "                                    stateful,\n",
        "                                    unroll,\n",
        "                                    **kwargs)\n",
        "    self.input_spec = [InputSpec(ndim=5)]\n",
        "    self.states = None\n",
        "    self._num_constants = None\n",
        "\n",
        "  @tf_utils.shape_type_conversion\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if isinstance(input_shape, list):\n",
        "      input_shape = input_shape[0]\n",
        "\n",
        "    cell = self.cell\n",
        "    if cell.data_format == 'channels_first':\n",
        "      rows = input_shape[3]\n",
        "      cols = input_shape[4]\n",
        "    elif cell.data_format == 'channels_last':\n",
        "      rows = input_shape[2]\n",
        "      cols = input_shape[3]\n",
        "    rows = conv_utils.conv_output_length(rows,\n",
        "                                         cell.kernel_size[0],\n",
        "                                         padding=cell.padding,\n",
        "                                         stride=cell.strides[0],\n",
        "                                         dilation=cell.dilation_rate[0])\n",
        "    cols = conv_utils.conv_output_length(cols,\n",
        "                                         cell.kernel_size[1],\n",
        "                                         padding=cell.padding,\n",
        "                                         stride=cell.strides[1],\n",
        "                                         dilation=cell.dilation_rate[1])\n",
        "\n",
        "    if cell.data_format == 'channels_first':\n",
        "      output_shape = input_shape[:2] + (cell.filters, rows, cols)\n",
        "    elif cell.data_format == 'channels_last':\n",
        "      output_shape = input_shape[:2] + (rows, cols, cell.filters)\n",
        "\n",
        "    if not self.return_sequences:\n",
        "      output_shape = output_shape[:1] + output_shape[2:]\n",
        "\n",
        "    if self.return_state:\n",
        "      output_shape = [output_shape]\n",
        "      if cell.data_format == 'channels_first':\n",
        "        output_shape += [(input_shape[0], cell.filters, rows, cols)\n",
        "                         for _ in range(2)]\n",
        "      elif cell.data_format == 'channels_last':\n",
        "        output_shape += [(input_shape[0], rows, cols, cell.filters)\n",
        "                         for _ in range(2)]\n",
        "    return output_shape\n",
        "\n",
        "  @tf_utils.shape_type_conversion\n",
        "  def build(self, input_shape):\n",
        "    # Note input_shape will be list of shapes of initial states and\n",
        "    # constants if these are passed in __call__.\n",
        "    if self._num_constants is not None:\n",
        "      constants_shape = input_shape[-self._num_constants:]  # pylint: disable=E1130\n",
        "    else:\n",
        "      constants_shape = None\n",
        "\n",
        "    if isinstance(input_shape, list):\n",
        "      input_shape = input_shape[0]\n",
        "\n",
        "    batch_size = input_shape[0] if self.stateful else None\n",
        "    self.input_spec[0] = InputSpec(shape=(batch_size, None) + input_shape[2:5])\n",
        "\n",
        "    # allow cell (if layer) to build before we set or validate state_spec\n",
        "    if isinstance(self.cell, Layer):\n",
        "      step_input_shape = (input_shape[0],) + input_shape[2:]\n",
        "      if constants_shape is not None:\n",
        "        self.cell.build([step_input_shape] + constants_shape)\n",
        "      else:\n",
        "        self.cell.build(step_input_shape)\n",
        "\n",
        "    # set or validate state_spec\n",
        "    if hasattr(self.cell.state_size, '__len__'):\n",
        "      state_size = list(self.cell.state_size)\n",
        "    else:\n",
        "      state_size = [self.cell.state_size]\n",
        "\n",
        "    if self.state_spec is not None:\n",
        "      # initial_state was passed in call, check compatibility\n",
        "      if self.cell.data_format == 'channels_first':\n",
        "        ch_dim = 1\n",
        "      elif self.cell.data_format == 'channels_last':\n",
        "        ch_dim = 3\n",
        "      if [spec.shape[ch_dim] for spec in self.state_spec] != state_size:\n",
        "        raise ValueError(\n",
        "            'An initial_state was passed that is not compatible with '\n",
        "            '`cell.state_size`. Received `state_spec`={}; '\n",
        "            'However `cell.state_size` is '\n",
        "            '{}'.format([spec.shape for spec in self.state_spec],\n",
        "                        self.cell.state_size))\n",
        "    else:\n",
        "      if self.cell.data_format == 'channels_first':\n",
        "        self.state_spec = [InputSpec(shape=(None, dim, None, None))\n",
        "                           for dim in state_size]\n",
        "      elif self.cell.data_format == 'channels_last':\n",
        "        self.state_spec = [InputSpec(shape=(None, None, None, dim))\n",
        "                           for dim in state_size]\n",
        "    if self.stateful:\n",
        "      self.reset_states()\n",
        "    self.built = True\n",
        "\n",
        "  def get_initial_state(self, inputs):\n",
        "    # (samples, timesteps, rows, cols, filters)\n",
        "    initial_state = K.zeros_like(inputs)\n",
        "    # (samples, rows, cols, filters)\n",
        "    initial_state = K.sum(initial_state, axis=1)\n",
        "    shape = list(self.cell.kernel_shape)\n",
        "    shape[-1] = self.cell.filters\n",
        "    initial_state = self.cell.input_conv(initial_state,\n",
        "                                         array_ops.zeros(tuple(shape)),\n",
        "                                         padding=self.cell.padding)\n",
        "\n",
        "    if hasattr(self.cell.state_size, '__len__'):\n",
        "      return [initial_state for _ in self.cell.state_size]\n",
        "    else:\n",
        "      return [initial_state]\n",
        "\n",
        "  def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n",
        "    inputs, initial_state, constants = _standardize_args(\n",
        "        inputs, initial_state, constants, self._num_constants)\n",
        "\n",
        "    if initial_state is None and constants is None:\n",
        "      return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n",
        "\n",
        "    # If any of `initial_state` or `constants` are specified and are Keras\n",
        "    # tensors, then add them to the inputs and temporarily modify the\n",
        "    # input_spec to include them.\n",
        "\n",
        "    additional_inputs = []\n",
        "    additional_specs = []\n",
        "    if initial_state is not None:\n",
        "      kwargs['initial_state'] = initial_state\n",
        "      additional_inputs += initial_state\n",
        "      self.state_spec = []\n",
        "      for state in initial_state:\n",
        "        shape = K.int_shape(state)\n",
        "        self.state_spec.append(InputSpec(shape=shape))\n",
        "\n",
        "      additional_specs += self.state_spec\n",
        "    if constants is not None:\n",
        "      kwargs['constants'] = constants\n",
        "      additional_inputs += constants\n",
        "      self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n",
        "                             for constant in constants]\n",
        "      self._num_constants = len(constants)\n",
        "      additional_specs += self.constants_spec\n",
        "    # at this point additional_inputs cannot be empty\n",
        "    for tensor in additional_inputs:\n",
        "      if K.is_keras_tensor(tensor) != K.is_keras_tensor(additional_inputs[0]):\n",
        "        raise ValueError('The initial state or constants of an RNN'\n",
        "                         ' layer cannot be specified with a mix of'\n",
        "                         ' Keras tensors and non-Keras tensors')\n",
        "\n",
        "    if K.is_keras_tensor(additional_inputs[0]):\n",
        "      # Compute the full input spec, including state and constants\n",
        "      full_input = [inputs] + additional_inputs\n",
        "      full_input_spec = self.input_spec + additional_specs\n",
        "      # Perform the call with temporarily replaced input_spec\n",
        "      original_input_spec = self.input_spec\n",
        "      self.input_spec = full_input_spec\n",
        "      output = super(ConvRNN2D, self).__call__(full_input, **kwargs)\n",
        "      self.input_spec = original_input_spec\n",
        "      return output\n",
        "    else:\n",
        "      return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n",
        "\n",
        "  def call(self,\n",
        "           inputs,\n",
        "           mask=None,\n",
        "           training=None,\n",
        "           initial_state=None,\n",
        "           constants=None):\n",
        "    # note that the .build() method of subclasses MUST define\n",
        "    # self.input_spec and self.state_spec with complete input shapes.\n",
        "    if isinstance(inputs, list):\n",
        "      inputs = inputs[0]\n",
        "    if initial_state is not None:\n",
        "      pass\n",
        "    elif self.stateful:\n",
        "      initial_state = self.states\n",
        "    else:\n",
        "      initial_state = self.get_initial_state(inputs)\n",
        "\n",
        "    if isinstance(mask, list):\n",
        "      mask = mask[0]\n",
        "\n",
        "    if len(initial_state) != len(self.states):\n",
        "      raise ValueError('Layer has ' + str(len(self.states)) +\n",
        "                       ' states but was passed ' +\n",
        "                       str(len(initial_state)) +\n",
        "                       ' initial states.')\n",
        "    timesteps = K.int_shape(inputs)[1]\n",
        "\n",
        "    kwargs = {}\n",
        "    if generic_utils.has_arg(self.cell.call, 'training'):\n",
        "      kwargs['training'] = training\n",
        "\n",
        "    if constants:\n",
        "      if not generic_utils.has_arg(self.cell.call, 'constants'):\n",
        "        raise ValueError('RNN cell does not support constants')\n",
        "\n",
        "      def step(inputs, states):\n",
        "        constants = states[-self._num_constants:]\n",
        "        states = states[:-self._num_constants]\n",
        "        return self.cell.call(inputs, states, constants=constants,\n",
        "                              **kwargs)\n",
        "    else:\n",
        "      def step(inputs, states):\n",
        "        return self.cell.call(inputs, states, **kwargs)\n",
        "\n",
        "    last_output, outputs, states = K.rnn(step,\n",
        "                                         inputs,\n",
        "                                         initial_state,\n",
        "                                         constants=constants,\n",
        "                                         go_backwards=self.go_backwards,\n",
        "                                         mask=mask,\n",
        "                                         input_length=timesteps)\n",
        "    if self.stateful:\n",
        "      updates = []\n",
        "      for i in range(len(states)):\n",
        "        updates.append(K.update(self.states[i], states[i]))\n",
        "      self.add_update(updates)\n",
        "\n",
        "    if self.return_sequences:\n",
        "      output = outputs\n",
        "    else:\n",
        "      output = last_output\n",
        "\n",
        "    if self.return_state:\n",
        "      if not isinstance(states, (list, tuple)):\n",
        "        states = [states]\n",
        "      else:\n",
        "        states = list(states)\n",
        "      return [output] + states\n",
        "    else:\n",
        "      return output\n",
        "\n",
        "  def reset_states(self, states=None):\n",
        "    if not self.stateful:\n",
        "      raise AttributeError('Layer must be stateful.')\n",
        "    input_shape = self.input_spec[0].shape\n",
        "    state_shape = self.compute_output_shape(input_shape)\n",
        "    if self.return_state:\n",
        "      state_shape = state_shape[0]\n",
        "    if self.return_sequences:\n",
        "      state_shape = state_shape[:1].concatenate(state_shape[2:])\n",
        "    if None in state_shape:\n",
        "      raise ValueError('If a RNN is stateful, it needs to know '\n",
        "                       'its batch size. Specify the batch size '\n",
        "                       'of your input tensors: \\n'\n",
        "                       '- If using a Sequential model, '\n",
        "                       'specify the batch size by passing '\n",
        "                       'a `batch_input_shape` '\n",
        "                       'argument to your first layer.\\n'\n",
        "                       '- If using the functional API, specify '\n",
        "                       'the time dimension by passing a '\n",
        "                       '`batch_shape` argument to your Input layer.\\n'\n",
        "                       'The same thing goes for the number of rows and '\n",
        "                       'columns.')\n",
        "\n",
        "    # helper function\n",
        "    def get_tuple_shape(nb_channels):\n",
        "      result = list(state_shape)\n",
        "      if self.cell.data_format == 'channels_first':\n",
        "        result[1] = nb_channels\n",
        "      elif self.cell.data_format == 'channels_last':\n",
        "        result[3] = nb_channels\n",
        "      else:\n",
        "        raise KeyError\n",
        "      return tuple(result)\n",
        "\n",
        "    # initialize state if None\n",
        "    if self.states[0] is None:\n",
        "      if hasattr(self.cell.state_size, '__len__'):\n",
        "        self.states = [K.zeros(get_tuple_shape(dim))\n",
        "                       for dim in self.cell.state_size]\n",
        "      else:\n",
        "        self.states = [K.zeros(get_tuple_shape(self.cell.state_size))]\n",
        "    elif states is None:\n",
        "      if hasattr(self.cell.state_size, '__len__'):\n",
        "        for state, dim in zip(self.states, self.cell.state_size):\n",
        "          K.set_value(state, np.zeros(get_tuple_shape(dim)))\n",
        "      else:\n",
        "        K.set_value(self.states[0],\n",
        "                    np.zeros(get_tuple_shape(self.cell.state_size)))\n",
        "    else:\n",
        "      if not isinstance(states, (list, tuple)):\n",
        "        states = [states]\n",
        "      if len(states) != len(self.states):\n",
        "        raise ValueError('Layer ' + self.name + ' expects ' +\n",
        "                         str(len(self.states)) + ' states, ' +\n",
        "                         'but it received ' + str(len(states)) +\n",
        "                         ' state values. Input received: ' + str(states))\n",
        "      for index, (value, state) in enumerate(zip(states, self.states)):\n",
        "        if hasattr(self.cell.state_size, '__len__'):\n",
        "          dim = self.cell.state_size[index]\n",
        "        else:\n",
        "          dim = self.cell.state_size\n",
        "        if value.shape != get_tuple_shape(dim):\n",
        "          raise ValueError('State ' + str(index) +\n",
        "                           ' is incompatible with layer ' +\n",
        "                           self.name + ': expected shape=' +\n",
        "                           str(get_tuple_shape(dim)) +\n",
        "                           ', found shape=' + str(value.shape))\n",
        "        # TODO(anjalisridhar): consider batch calls to `set_value`.\n",
        "        K.set_value(state, value)\n",
        "\n",
        "\n",
        "class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n",
        "  \"\"\"Cell class for the ConvLSTM2D layer.\n",
        "  Arguments:\n",
        "    filters: Integer, the dimensionality of the output space\n",
        "      (i.e. the number of output filters in the convolution).\n",
        "    kernel_size: An integer or tuple/list of n integers, specifying the\n",
        "      dimensions of the convolution window.\n",
        "    strides: An integer or tuple/list of n integers,\n",
        "      specifying the strides of the convolution.\n",
        "      Specifying any stride value != 1 is incompatible with specifying\n",
        "      any `dilation_rate` value != 1.\n",
        "    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
        "    data_format: A string,\n",
        "      one of `channels_last` (default) or `channels_first`.\n",
        "      It defaults to the `image_data_format` value found in your\n",
        "      Keras config file at `~/.keras/keras.json`.\n",
        "      If you never set it, then it will be \"channels_last\".\n",
        "    dilation_rate: An integer or tuple/list of n integers, specifying\n",
        "      the dilation rate to use for dilated convolution.\n",
        "      Currently, specifying any `dilation_rate` value != 1 is\n",
        "      incompatible with specifying any `strides` value != 1.\n",
        "    activation: Activation function to use.\n",
        "      If you don't specify anything, no activation is applied\n",
        "      (ie. \"linear\" activation: `a(x) = x`).\n",
        "    recurrent_activation: Activation function to use\n",
        "      for the recurrent step.\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "      used for the linear transformation of the inputs.\n",
        "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
        "      weights matrix,\n",
        "      used for the linear transformation of the recurrent state.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    unit_forget_bias: Boolean.\n",
        "      If True, add 1 to the bias of the forget gate at initialization.\n",
        "      Use in combination with `bias_initializer=\"zeros\"`.\n",
        "      This is recommended in [Jozefowicz et al.]\n",
        "      (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
        "    kernel_regularizer: Regularizer function applied to\n",
        "      the `kernel` weights matrix.\n",
        "    recurrent_regularizer: Regularizer function applied to\n",
        "      the `recurrent_kernel` weights matrix.\n",
        "    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "    kernel_constraint: Constraint function applied to\n",
        "      the `kernel` weights matrix.\n",
        "    recurrent_constraint: Constraint function applied to\n",
        "      the `recurrent_kernel` weights matrix.\n",
        "    bias_constraint: Constraint function applied to the bias vector.\n",
        "    dropout: Float between 0 and 1.\n",
        "      Fraction of the units to drop for\n",
        "      the linear transformation of the inputs.\n",
        "    recurrent_dropout: Float between 0 and 1.\n",
        "      Fraction of the units to drop for\n",
        "      the linear transformation of the recurrent state.\n",
        "  Call arguments:\n",
        "    inputs: A 4D tensor.\n",
        "    states:  List of state tensors corresponding to the previous timestep.\n",
        "    training: Python boolean indicating whether the layer should behave in\n",
        "      training mode or in inference mode. Only relevant when `dropout` or\n",
        "      `recurrent_dropout` is used.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation='tanh',\n",
        "               recurrent_activation='hard_sigmoid',\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               recurrent_initializer='orthogonal',\n",
        "               bias_initializer='zeros',\n",
        "               unit_forget_bias=True,\n",
        "               kernel_regularizer=None,\n",
        "               recurrent_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               recurrent_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               dropout=0.,\n",
        "               recurrent_dropout=0.,\n",
        "               **kwargs):\n",
        "    super(ConvLSTM2DCell, self).__init__(**kwargs)\n",
        "    self.filters = filters\n",
        "    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n",
        "    self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n",
        "    self.padding = conv_utils.normalize_padding(padding)\n",
        "    self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "    self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n",
        "                                                    'dilation_rate')\n",
        "    self.activation = activations.get(activation)\n",
        "    self.recurrent_activation = activations.get(recurrent_activation)\n",
        "    self.use_bias = use_bias\n",
        "\n",
        "    self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "    self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "    self.bias_initializer = initializers.get(bias_initializer)\n",
        "    self.unit_forget_bias = unit_forget_bias\n",
        "\n",
        "    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "    self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "    self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "    self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "    self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "    self.dropout = min(1., max(0., dropout))\n",
        "    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "    self.state_size = (self.filters, self.filters)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "\n",
        "    if self.data_format == 'channels_first':\n",
        "      channel_axis = 1\n",
        "    else:\n",
        "      channel_axis = -1\n",
        "    if input_shape[channel_axis] is None:\n",
        "      raise ValueError('The channel dimension of the inputs '\n",
        "                       'should be defined. Found `None`.')\n",
        "    input_dim = input_shape[channel_axis]\n",
        "    kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n",
        "    self.kernel_shape = kernel_shape\n",
        "    recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n",
        "\n",
        "    self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='kernel',\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "    self.recurrent_kernel = self.add_weight(\n",
        "        shape=recurrent_kernel_shape,\n",
        "        initializer=self.recurrent_initializer,\n",
        "        name='recurrent_kernel',\n",
        "        regularizer=self.recurrent_regularizer,\n",
        "        constraint=self.recurrent_constraint)\n",
        "\n",
        "    if self.use_bias:\n",
        "      if self.unit_forget_bias:\n",
        "\n",
        "        def bias_initializer(_, *args, **kwargs):\n",
        "          return K.concatenate([\n",
        "              self.bias_initializer((self.filters,), *args, **kwargs),\n",
        "              initializers.Ones()((self.filters,), *args, **kwargs),\n",
        "              self.bias_initializer((self.filters * 2,), *args, **kwargs),\n",
        "          ])\n",
        "      else:\n",
        "        bias_initializer = self.bias_initializer\n",
        "      self.bias = self.add_weight(\n",
        "          shape=(self.filters * 4,),\n",
        "          name='bias',\n",
        "          initializer=bias_initializer,\n",
        "          regularizer=self.bias_regularizer,\n",
        "          constraint=self.bias_constraint)\n",
        "    else:\n",
        "      self.bias = None\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, states, training=None):\n",
        "    h_tm1 = states[0]  # previous memory state\n",
        "    c_tm1 = states[1]  # previous carry state\n",
        "\n",
        "    # dropout matrices for input units\n",
        "    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n",
        "    # dropout matrices for recurrent units\n",
        "    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
        "        h_tm1, training, count=4)\n",
        "\n",
        "    if 0 < self.dropout < 1.:\n",
        "      inputs_i = inputs * dp_mask[0]\n",
        "      inputs_f = inputs * dp_mask[1]\n",
        "      inputs_c = inputs * dp_mask[2]\n",
        "      inputs_o = inputs * dp_mask[3]\n",
        "    else:\n",
        "      inputs_i = inputs\n",
        "      inputs_f = inputs\n",
        "      inputs_c = inputs\n",
        "      inputs_o = inputs\n",
        "\n",
        "    if 0 < self.recurrent_dropout < 1.:\n",
        "      h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
        "      h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
        "      h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
        "      h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
        "    else:\n",
        "      h_tm1_i = h_tm1\n",
        "      h_tm1_f = h_tm1\n",
        "      h_tm1_c = h_tm1\n",
        "      h_tm1_o = h_tm1\n",
        "\n",
        "    (kernel_i, kernel_f,\n",
        "     kernel_c, kernel_o) = array_ops.split(self.kernel, 4, axis=3)\n",
        "    (recurrent_kernel_i,\n",
        "     recurrent_kernel_f,\n",
        "     recurrent_kernel_c,\n",
        "     recurrent_kernel_o) = array_ops.split(self.recurrent_kernel, 4, axis=3)\n",
        "\n",
        "    if self.use_bias:\n",
        "      bias_i, bias_f, bias_c, bias_o = array_ops.split(self.bias, 4)\n",
        "    else:\n",
        "      bias_i, bias_f, bias_c, bias_o = None, None, None, None\n",
        "\n",
        "    x_i = self.input_conv(inputs_i, kernel_i, bias_i, padding=self.padding)\n",
        "    x_f = self.input_conv(inputs_f, kernel_f, bias_f, padding=self.padding)\n",
        "    x_c = self.input_conv(inputs_c, kernel_c, bias_c, padding=self.padding)\n",
        "    x_o = self.input_conv(inputs_o, kernel_o, bias_o, padding=self.padding)\n",
        "    h_i = self.recurrent_conv(h_tm1_i, recurrent_kernel_i)\n",
        "    h_f = self.recurrent_conv(h_tm1_f, recurrent_kernel_f)\n",
        "    h_c = self.recurrent_conv(h_tm1_c, recurrent_kernel_c)\n",
        "    h_o = self.recurrent_conv(h_tm1_o, recurrent_kernel_o)\n",
        "\n",
        "    i = self.recurrent_activation(x_i + h_i)\n",
        "    f = self.recurrent_activation(x_f + h_f)\n",
        "    c = f * c_tm1 + i * self.activation(x_c + h_c)\n",
        "    o = self.recurrent_activation(x_o + h_o)\n",
        "    h = o * self.activation(c)\n",
        "    return h, [h, c]\n",
        "\n",
        "  def input_conv(self, x, w, b=None, padding='valid'):\n",
        "    conv_out = K.conv2d(x, w, strides=self.strides,\n",
        "                        padding=padding,\n",
        "                        data_format=self.data_format,\n",
        "                        dilation_rate=self.dilation_rate)\n",
        "    if b is not None:\n",
        "      conv_out = K.bias_add(conv_out, b,\n",
        "                            data_format=self.data_format)\n",
        "    return conv_out\n",
        "\n",
        "  def recurrent_conv(self, x, w):\n",
        "    conv_out = K.conv2d(x, w, strides=(1, 1),\n",
        "                        padding='same',\n",
        "                        data_format=self.data_format)\n",
        "    return conv_out\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {'filters': self.filters,\n",
        "              'kernel_size': self.kernel_size,\n",
        "              'strides': self.strides,\n",
        "              'padding': self.padding,\n",
        "              'data_format': self.data_format,\n",
        "              'dilation_rate': self.dilation_rate,\n",
        "              'activation': activations.serialize(self.activation),\n",
        "              'recurrent_activation': activations.serialize(\n",
        "                  self.recurrent_activation),\n",
        "              'use_bias': self.use_bias,\n",
        "              'kernel_initializer': initializers.serialize(\n",
        "                  self.kernel_initializer),\n",
        "              'recurrent_initializer': initializers.serialize(\n",
        "                  self.recurrent_initializer),\n",
        "              'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "              'unit_forget_bias': self.unit_forget_bias,\n",
        "              'kernel_regularizer': regularizers.serialize(\n",
        "                  self.kernel_regularizer),\n",
        "              'recurrent_regularizer': regularizers.serialize(\n",
        "                  self.recurrent_regularizer),\n",
        "              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "              'kernel_constraint': constraints.serialize(\n",
        "                  self.kernel_constraint),\n",
        "              'recurrent_constraint': constraints.serialize(\n",
        "                  self.recurrent_constraint),\n",
        "              'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "              'dropout': self.dropout,\n",
        "              'recurrent_dropout': self.recurrent_dropout}\n",
        "    base_config = super(ConvLSTM2DCell, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPa4mqOMtOWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPSILON = 0.00001\n",
        "def tensor_layer_norm(x, state_name):\n",
        "    x_shape = x.get_shape()\n",
        "    dims = x_shape.ndims\n",
        "    params_shape = x_shape[-1:]\n",
        "    if dims == 4:\n",
        "        m, v = tf.nn.moments(x, [1,2,3], keep_dims=True)\n",
        "    elif dims == 5:\n",
        "        m, v = tf.nn.moments(x, [1,2,3,4], keep_dims=True)\n",
        "    else:\n",
        "        raise ValueError('input tensor for layer normalization must be rank 4 or 5.')\n",
        "    b = tf.get_variable(state_name+'b',initializer=tf.zeros(params_shape))\n",
        "    s = tf.get_variable(state_name+'s',initializer=tf.ones(params_shape))\n",
        "    x_tln = tf.nn.batch_normalization(x, m, v, b, s, EPSILON)\n",
        "    return x_tln"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ30NUb7772z",
        "colab_type": "text"
      },
      "source": [
        "# CausalLSTMCell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3km7gr2mAlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalLSTMCell(DropoutRNNCellMixin, Layer):\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               forget_bias=1.0,\n",
        "               tln=False,\n",
        "               initializer=0.001,\n",
        "               layer_name=\"causal_lstm\",\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation='tanh',\n",
        "               recurrent_activation='hard_sigmoid',\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               recurrent_initializer='orthogonal',\n",
        "               bias_initializer='zeros',\n",
        "               unit_forget_bias=True,\n",
        "               kernel_regularizer=None,\n",
        "               recurrent_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               recurrent_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               dropout=0.,\n",
        "               recurrent_dropout=0.,\n",
        "               **kwargs):\n",
        "        super(CausalLSTMCell, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.num_hidden = filters\n",
        "        self.filter_size = kernel_size[0]\n",
        "        self.layer_norm = tln\n",
        "        self._forget_bias = forget_bias\n",
        "        self.initializer = tf.random_uniform_initializer(-initializer,initializer)\n",
        "        self.layer_name = layer_name\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
        "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n",
        "                                                        'dilation_rate')\n",
        "        self.activation = activations.get(activation)\n",
        "        self.recurrent_activation = activations.get(recurrent_activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.unit_forget_bias = unit_forget_bias\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = min(1., max(0., dropout))\n",
        "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
        "        self.state_size = (self.filters, self.filters)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                        'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n",
        "        self.kernel_shape = kernel_shape\n",
        "        recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n",
        "\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                    initializer=self.kernel_initializer,\n",
        "                                    name='kernel',\n",
        "                                    regularizer=self.kernel_regularizer,\n",
        "                                    constraint=self.kernel_constraint)\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=recurrent_kernel_shape,\n",
        "            initializer=self.recurrent_initializer,\n",
        "            name='recurrent_kernel',\n",
        "            regularizer=self.recurrent_regularizer,\n",
        "            constraint=self.recurrent_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.unit_forget_bias:\n",
        "\n",
        "                def bias_initializer(_, *args, **kwargs):\n",
        "                    return K.concatenate([\n",
        "                        self.bias_initializer((self.filters,), *args, **kwargs),\n",
        "                        initializers.Ones()((self.filters,), *args, **kwargs),\n",
        "                        self.bias_initializer((self.filters * 2,), *args, **kwargs),\n",
        "                     ])\n",
        "            else:\n",
        "                bias_initializer = self.bias_initializer\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(self.filters * 4,),\n",
        "                name='bias',\n",
        "                initializer=bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x, states):\n",
        "        h = states[0]\n",
        "        c = states[0]\n",
        "        m = states[1]\n",
        "        self.batch = x.shape[0]\n",
        "        self.height = x.shape[2]\n",
        "        self.width = x.shape[3]\n",
        "        if h is None:\n",
        "            h = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden],\n",
        "                         dtype=tf.float32)\n",
        "        if c is None:\n",
        "            c = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden],\n",
        "                         dtype=tf.float32)\n",
        "        if m is None:\n",
        "            m = tf.zeros([self.batch, self.height, self.width,\n",
        "                          self.num_hidden_in],\n",
        "                         dtype=tf.float32)\n",
        "\n",
        "        #with tf.variable_scope(self.layer_name):\n",
        "        h_cc = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*4,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='temporal_state_transition')(h)\n",
        "        c_cc = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*3,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='temporal_memory_transition')(c)\n",
        "        m_cc = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*3,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='spatial_memory_transition')(m)\n",
        "        if self.layer_norm:\n",
        "            h_cc = tensor_layer_norm(h_cc, 'h2c')\n",
        "            c_cc = tensor_layer_norm(c_cc, 'c2c')\n",
        "            m_cc = tensor_layer_norm(m_cc, 'm2m')\n",
        "\n",
        "        i_h, g_h, f_h, o_h = tf.split(h_cc, 4, 3)\n",
        "        i_c, g_c, f_c = tf.split(c_cc, 3, 3)\n",
        "        i_m, f_m, m_m = tf.split(m_cc, 3, 3)\n",
        "\n",
        "        if x is None:\n",
        "            i = tf.sigmoid(i_h + i_c)\n",
        "            f = tf.sigmoid(f_h + f_c + self._forget_bias)\n",
        "            g = tf.tanh(g_h + g_c)\n",
        "        else:\n",
        "            x_cc = tf.keras.layers.Conv2D(\n",
        "                self.num_hidden*7,\n",
        "                self.filter_size, 1, padding=self.padding,\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='input_to_state')(x)\n",
        "            if self.layer_norm:\n",
        "                x_cc = tensor_layer_norm(x_cc, 'x2c')\n",
        "\n",
        "            i_x, g_x, f_x, o_x, i_x_, g_x_, f_x_ = tf.split(x_cc, 7, 3)\n",
        "\n",
        "            i = tf.sigmoid(i_x + i_h + i_c)\n",
        "            f = tf.sigmoid(f_x + f_h + f_c + self._forget_bias)\n",
        "            g = tf.tanh(g_x + g_h + g_c)\n",
        "\n",
        "        c_new = f * c + i * g\n",
        "\n",
        "        c2m = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden*4,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='c2m')(c_new)\n",
        "        if self.layer_norm:\n",
        "            c2m = tensor_layer_norm(c2m, 'c2m')\n",
        "\n",
        "        i_c, g_c, f_c, o_c = tf.split(c2m, 4, 3)\n",
        "\n",
        "        if x is None:\n",
        "            ii = tf.sigmoid(i_c + i_m)\n",
        "            ff = tf.sigmoid(f_c + f_m + self._forget_bias)\n",
        "            gg = tf.tanh(g_c)\n",
        "        else:\n",
        "            ii = tf.sigmoid(i_c + i_x_ + i_m)\n",
        "            ff = tf.sigmoid(f_c + f_x_ + f_m + self._forget_bias)\n",
        "            gg = tf.tanh(g_c + g_x_)\n",
        "\n",
        "        m_new = ff * tf.tanh(m_m) + ii * gg\n",
        "\n",
        "        o_m = tf.keras.layers.Conv2D(\n",
        "            self.num_hidden,\n",
        "            self.filter_size, 1, padding=self.padding,\n",
        "            kernel_initializer=self.initializer,\n",
        "            name='m_to_o')(m_new)\n",
        "        if self.layer_norm:\n",
        "            o_m = tensor_layer_norm(o_m, 'm2o')\n",
        "\n",
        "        if x is None:\n",
        "            o = tf.tanh(o_h + o_c + o_m)\n",
        "        else:\n",
        "            o = tf.tanh(o_x + o_h + o_c + o_m)\n",
        "\n",
        "        cell = tf.concat([c_new, m_new],-1)\n",
        "        cell = tf.keras.layers.Conv2D(self.num_hidden, 1, 1,\n",
        "                                padding=self.padding, name='memory_reduce')(cell)\n",
        "        print(cell)\n",
        "        h_new = o * tf.tanh(cell)\n",
        "\n",
        "        #return h_new, c_new, m_new\n",
        "        return h_new, [c_new, m_new]\n",
        "\n",
        "    def input_conv(self, x, w, b=None, padding='valid'):\n",
        "        conv_out = K.conv2d(x, w, strides=self.strides,\n",
        "                            padding=padding,\n",
        "                            data_format=self.data_format,\n",
        "                            dilation_rate=self.dilation_rate)\n",
        "        if b is not None:\n",
        "            conv_out = K.bias_add(conv_out, b,\n",
        "                                data_format=self.data_format)\n",
        "        return conv_out\n",
        "\n",
        "    def recurrent_conv(self, x, w):\n",
        "        conv_out = K.conv2d(x, w, strides=(1, 1),\n",
        "                            padding='same',\n",
        "                            data_format=self.data_format)\n",
        "        return conv_out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'filters': self.filters,\n",
        "              'kernel_size': self.kernel_size,\n",
        "              'strides': self.strides,\n",
        "              'padding': self.padding,\n",
        "              'data_format': self.data_format,\n",
        "              'dilation_rate': self.dilation_rate,\n",
        "              'activation': activations.serialize(self.activation),\n",
        "              'recurrent_activation': activations.serialize(\n",
        "                  self.recurrent_activation),\n",
        "              'use_bias': self.use_bias,\n",
        "              'kernel_initializer': initializers.serialize(\n",
        "                  self.kernel_initializer),\n",
        "              'recurrent_initializer': initializers.serialize(\n",
        "                  self.recurrent_initializer),\n",
        "              'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "              'unit_forget_bias': self.unit_forget_bias,\n",
        "              'kernel_regularizer': regularizers.serialize(\n",
        "                  self.kernel_regularizer),\n",
        "              'recurrent_regularizer': regularizers.serialize(\n",
        "                  self.recurrent_regularizer),\n",
        "              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "              'kernel_constraint': constraints.serialize(\n",
        "                  self.kernel_constraint),\n",
        "              'recurrent_constraint': constraints.serialize(\n",
        "                  self.recurrent_constraint),\n",
        "              'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "              'dropout': self.dropout,\n",
        "              'recurrent_dropout': self.recurrent_dropout}\n",
        "        base_config = super(ConvLSTM2DCell, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7kJYXNu738-",
        "colab_type": "text"
      },
      "source": [
        "# CausalLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKFr0R1ULjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@keras_export('keras.layers.CausalLSTM')\n",
        "class CausalLSTM(ConvRNN2D):\n",
        "\n",
        "    def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               layer_name='causal_lstm',\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation='tanh',\n",
        "               recurrent_activation='hard_sigmoid',\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               recurrent_initializer='orthogonal',\n",
        "               bias_initializer='zeros',\n",
        "               unit_forget_bias=True,\n",
        "               kernel_regularizer=None,\n",
        "               recurrent_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               recurrent_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               return_sequences=False,\n",
        "               go_backwards=False,\n",
        "               stateful=False,\n",
        "               dropout=0.,\n",
        "               recurrent_dropout=0.,\n",
        "               **kwargs):\n",
        "        num_hidden = filters\n",
        "        filter_size = kernel_size[0]\n",
        "        cell = CausalLSTMCell(\n",
        "                        filters=filters,\n",
        "                          kernel_size=kernel_size,\n",
        "                          strides=strides,\n",
        "                          padding=padding,\n",
        "                          data_format=data_format,\n",
        "                          dilation_rate=dilation_rate,\n",
        "                          activation=activation,\n",
        "                          recurrent_activation=recurrent_activation,\n",
        "                          use_bias=use_bias,\n",
        "                          kernel_initializer=kernel_initializer,\n",
        "                          layer_name=layer_name,\n",
        "                          recurrent_initializer=recurrent_initializer,\n",
        "                          bias_initializer=bias_initializer,\n",
        "                          unit_forget_bias=unit_forget_bias,\n",
        "                          kernel_regularizer=kernel_regularizer,\n",
        "                          recurrent_regularizer=recurrent_regularizer,\n",
        "                          bias_regularizer=bias_regularizer,\n",
        "                          kernel_constraint=kernel_constraint,\n",
        "                          recurrent_constraint=recurrent_constraint,\n",
        "                          bias_constraint=bias_constraint,\n",
        "                          dropout=dropout,\n",
        "                          recurrent_dropout=recurrent_dropout,\n",
        "                          dtype=kwargs.get('dtype')\n",
        "        )\n",
        "        super(CausalLSTM, self).__init__(cell,\n",
        "                                         return_sequences=return_sequences,\n",
        "                                         go_backwards=go_backwards,\n",
        "                                         stateful=stateful,\n",
        "                                         **kwargs)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        \n",
        "        def call(self, inputs, mask=None, training=None, initial_state=None):\n",
        "            return super (CausalLSTM, self).call(inputs,\n",
        "                                                 mask=mask,\n",
        "                                                 training=training,\n",
        "                                                 initial_state=initial_state)\n",
        "\n",
        "        @property\n",
        "        def filters(self):\n",
        "            return self.cell.filters\n",
        "\n",
        "        @property\n",
        "        def kernel_size(self):\n",
        "            return self.cell.kernel_size\n",
        "\n",
        "        @property\n",
        "        def strides(self):\n",
        "            return self.cell.strides\n",
        "\n",
        "        @property\n",
        "        def padding(self):\n",
        "            return self.cell.padding\n",
        "\n",
        "        @property\n",
        "        def data_format(self):\n",
        "            return self.cell.data_format\n",
        "\n",
        "        @property\n",
        "        def dilation_rate(self):\n",
        "            return self.cell.dilation_rate\n",
        "\n",
        "        @property\n",
        "        def activation(self):\n",
        "            return self.cell.activation\n",
        "\n",
        "        @property\n",
        "        def recurrent_activation(self):\n",
        "            return self.cell.recurrent_activation\n",
        "\n",
        "        @property\n",
        "        def use_bias(self):\n",
        "            return self.cell.use_bias\n",
        "\n",
        "        @property\n",
        "        def kernel_initializer(self):\n",
        "            return self.cell.kernel_initializer\n",
        "\n",
        "        @property\n",
        "        def recurrent_initializer(self):\n",
        "            return self.cell.recurrent_initializer\n",
        "\n",
        "        @property\n",
        "        def bias_initializer(self):\n",
        "            return self.cell.bias_initializer\n",
        "\n",
        "        @property\n",
        "        def unit_forget_bias(self):\n",
        "            return self.cell.unit_forget_bias\n",
        "\n",
        "        @property\n",
        "        def kernel_regularizer(self):\n",
        "            return self.cell.kernel_regularizer\n",
        "\n",
        "        @property\n",
        "        def recurrent_regularizer(self):\n",
        "            return self.cell.recurrent_regularizer\n",
        "\n",
        "        @property\n",
        "        def bias_regularizer(self):\n",
        "            return self.cell.bias_regularizer\n",
        "\n",
        "        @property\n",
        "        def kernel_constraint(self):\n",
        "            return self.cell.kernel_constraint\n",
        "\n",
        "        @property\n",
        "        def recurrent_constraint(self):\n",
        "            return self.cell.recurrent_constraint\n",
        "\n",
        "        @property\n",
        "        def bias_constraint(self):\n",
        "            return self.cell.bias_constraint\n",
        "\n",
        "        @property\n",
        "        def dropout(self):\n",
        "            return self.cell.dropout\n",
        "\n",
        "        @property\n",
        "        def recurrent_dropout(self):\n",
        "            return self.cell.recurrent_dropout\n",
        "\n",
        "        def get_config(self):\n",
        "            config = {'filters': self.filters,\n",
        "                    'kernel_size': self.kernel_size,\n",
        "                    'strides': self.strides,\n",
        "                    'padding': self.padding,\n",
        "                    'data_format': self.data_format,\n",
        "                    'dilation_rate': self.dilation_rate,\n",
        "                    'activation': activations.serialize(self.activation),\n",
        "                    'recurrent_activation':\n",
        "                        activations.serialize(self.recurrent_activation),\n",
        "                    'use_bias': self.use_bias,\n",
        "                    'kernel_initializer':\n",
        "                        initializers.serialize(self.kernel_initializer),\n",
        "                    'recurrent_initializer':\n",
        "                        initializers.serialize(self.recurrent_initializer),\n",
        "                    'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                    'unit_forget_bias': self.unit_forget_bias,\n",
        "                    'kernel_regularizer':\n",
        "                        regularizers.serialize(self.kernel_regularizer),\n",
        "                    'recurrent_regularizer':\n",
        "                        regularizers.serialize(self.recurrent_regularizer),\n",
        "                    'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                    'activity_regularizer':\n",
        "                        regularizers.serialize(self.activity_regularizer),\n",
        "                    'kernel_constraint':\n",
        "                        constraints.serialize(self.kernel_constraint),\n",
        "                    'recurrent_constraint':\n",
        "                        constraints.serialize(self.recurrent_constraint),\n",
        "                    'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                    'dropout': self.dropout,\n",
        "                    'recurrent_dropout': self.recurrent_dropout}\n",
        "            base_config = super(ConvLSTM2D, self).get_config()\n",
        "            del base_config['cell']\n",
        "            return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "        @classmethod\n",
        "        def from_config(cls, config):\n",
        "            return cls(**config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqsB6ngL71Ac",
        "colab_type": "text"
      },
      "source": [
        "# GHU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjyncnnQNRTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GHU(keras.layers.Layer):\n",
        "    def __init__(self, layer_name, filter_size, num_features, tln=False,\n",
        "                 initializer=0.001, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \"\"\"Initialize the Gradient Highway Unit.\n",
        "        \"\"\"\n",
        "        self.layer_name = layer_name\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self.layer_norm = tln\n",
        "        if initializer == -1:\n",
        "            self.initializer = None\n",
        "        else:\n",
        "            self.initializer = tf.random_uniform_initializer(-initializer,initializer)\n",
        "\n",
        "    def init_state(self, inputs, num_features):\n",
        "        dims = inputs.get_shape().ndims\n",
        "        if dims == 4:\n",
        "            batch = inputs.get_shape()[0]\n",
        "            height = inputs.get_shape()[1]\n",
        "            width = inputs.get_shape()[2]\n",
        "        else:\n",
        "            raise ValueError('input tensor should be rank 4.')\n",
        "        return tf.zeros([batch, height, width, num_features], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x, z):\n",
        "        if z is None:\n",
        "            z = self.init_state(x, self.num_features)\n",
        "        with tf.variable_scope(self.layer_name):\n",
        "            z_concat = tf.keras.layers.Conv2D(\n",
        "                z, self.num_features*2,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='state_to_state')\n",
        "            if self.layer_norm:\n",
        "                z_concat = tensor_layer_norm(z_concat, 'state_to_state')\n",
        "\n",
        "            x_concat = tf.keras.layers.Conv2D(\n",
        "                x, self.num_features*2,\n",
        "                self.filter_size, 1, padding='same',\n",
        "                kernel_initializer=self.initializer,\n",
        "                name='input_to_state')\n",
        "            if self.layer_norm:\n",
        "                x_concat = tensor_layer_norm(x_concat, 'input_to_state')\n",
        "\n",
        "            gates = tf.add(x_concat, z_concat)\n",
        "            p, u = tf.split(gates, 2, 3)\n",
        "            p = tf.nn.tanh(p)\n",
        "            u = tf.nn.sigmoid(u)\n",
        "            z_new = u * p + (1-u) * z\n",
        "            return z_new\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgjmvA5reMx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Input\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAg30tTEvq9",
        "colab_type": "text"
      },
      "source": [
        "# Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNmcetj1eqjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw4dz7d-erHx",
        "colab_type": "code",
        "outputId": "2dbd6456-9599-4261-e7f2-ddc4020c076e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_set = '...'"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tipo di dato  <class 'numpy.ndarray'>\n",
            "Data shape:  (20, 10000, 64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVDywNbd5Lny",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAPpiZBBY1w-",
        "colab_type": "code",
        "outputId": "2a5e3f33-9bfe-4fdc-ab7d-aec6bd7ba7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_set = train_set.reshape([10000,20,64,64,1])\n",
        "train_set.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 20, 64, 64, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOwzdJL8E8rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = np.swapaxes(train_set,1,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d13Cf3soFY_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25e7fe51-2ad4-42a7-c7db-05366559bc29"
      },
      "source": [
        "train_set.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 20, 64, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrcCRiniEpMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "b3284b1d-339c-4f08-a4ae-28eb04962d65"
      },
      "source": [
        "plt.imshow(train_set[2,19,...,0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbbab7735f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo6CulzIXiut",
        "colab_type": "code",
        "outputId": "bbc0aef8-75e0-4644-c49e-1551832349a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "#from keras.models import Sequential\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(CausalLSTM(filters=1, #64\n",
        "                  kernel_size=(3,3),\n",
        "                  input_shape=(None, 64, 64, 1),\n",
        "                  padding='same', \n",
        "                  return_sequences=True))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
        "model.fit(train_set[:10], train_set[:10], batch_size=10,\n",
        "        epochs=300, validation_split=0.05)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"causal_lstm/memory_reduce/Identity:0\", shape=(None, 64, 64, 1), dtype=float32)\n",
            "Tensor(\"memory_reduce/Identity:0\", shape=(None, 64, 64, 1), dtype=float32)\n",
            "Train on 9 samples, validate on 1 samples\n",
            "Epoch 1/300\n",
            "Tensor(\"sequential/causal_lstm/memory_reduce/BiasAdd:0\", shape=(None, 64, 64, 1), dtype=float32)\n",
            "Tensor(\"memory_reduce/BiasAdd:0\", shape=(None, 64, 64, 1), dtype=float32)\n",
            "9/9 [==============================] - 1s 112ms/sample\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cbbefff5aff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adadelta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m model.fit(train_set[:10], train_set[:10], batch_size=10,\n\u001b[0;32m---> 10\u001b[0;31m         epochs=300, validation_split=0.05)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 408\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 73\u001b[0;31m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2132\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n\u001b[1;32m    271\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unscaled_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         logging.warning('The list of trainable weights is empty. Make sure that'\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1023\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m-> 1025\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m   1026\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     logging.warning(\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['causal_lstm/kernel:0', 'causal_lstm/recurrent_kernel:0', 'causal_lstm/bias:0']."
          ]
        }
      ]
    }
  ]
}